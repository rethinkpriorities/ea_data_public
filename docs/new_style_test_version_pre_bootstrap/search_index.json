[["index.html", "EA survey analyses (partial) 1 INDEX (skip)", " EA survey analyses (partial) Dr. David Reinstein, many others 2022-02-09 1 INDEX (skip) #From https://stackoverflow.com/questions/62028925/how-do-i-catch-errors-in-inline-code-chunk-in-r-markdown knitr::knit_hooks$set( evaluate.inline = function (code, envir = knit_global()) { v = try(eval(xfun::parse_only(code), envir = envir)) knitr::knit_print(v, inline = TRUE, options = knitr::opts_chunk$get()) }, inline = function(x) { if (any(class(x) == &quot;try-error&quot;)) { as.vector(x) } else x }) #TODO: Some of this repeats content in &#39;main&#39; --- delete from one or the other? #Define objects for 2020 survey year_s &lt;- &quot;2020&quot; year_n &lt;- 2020 #formatting functions op &lt;- function(x, d=3) format(x, format=&quot;f&quot;, big.mark=&quot;,&quot;, digits=d, scientific=FALSE) ops &lt;- function(x, d=3, ns=2) format(x, format=&quot;f&quot;, big.mark=&quot;,&quot;, digits=d, nsmall=ns, scientific = FALSE) options(scipen=999) #scatter_theme &lt;- theme_minimal() "],["outline_disc.html", "2 Notes on this ‘bookdown’ relating to the EA Survey", " 2 Notes on this ‘bookdown’ relating to the EA Survey Rmdnote thing below: This material is meant to supplement EA Survey series posts on the EA forum, and may be linked in those posts. It has been/is being put together by the Rethink Priorities survey team, (particular David Reinstein, Willem Sleegers, and Jamie Elsley, under the management of David Moss, with the collaboration and input from others at RP. Now testing a lonely footnote, will it hover?1 And testing a ‘foldable’ … these sometimes work, sometimes come out only as grey bars’: Show me more I might hope that it indeed does contain materials from multiple posts/chapters — imho that’s a key feature, not a bug. You can easily jump/link across these and also it gives people perspective. 2.0.1 Questions with folded answers, folded expanded discussions Can I ask you a question? (Unfold for answer) Yes: I just did. blah blah "],["eas_donations.html", "3 Donations 3.1 Introduction and summary 3.2 Total EA donations, magnitudes in context 3.3 Career paths: Earning-to-give 3.4 Donation totals: descriptives 3.5 Donation and income for recent years 3.6 Which charities (causes and categories) are EAs donating to? 3.7 Donations: plans/aspirations vs. actual (reported) amounts 3.8 Donations versus next year’s plans 3.9 Model of EA donation behavior 3.10 Appendix: Extra analysis and robustness checks", " 3 Donations Note: This is largely meant as a year-independent template. ‘…’ indicates a place we may want to fill in more details2 See our discussion of what should go in each chapter here. Note that we aim to keep the commentary here very limited. The EA forum post will do a bit more commenting… feel free to insert, but hide any notes on this output that we may want to integrate into the EA forum post. We will try to move all ‘generic’ statistical and methods discussions to our methods discussion book, and link this here. This report is summarized and discussed in the EA forum post … Linked to forum post: Link; EA Survey 2020 Series: Donation Data** # Folder to save plots in plots_folder &lt;- here(&quot;analysis&quot;, &quot;plots&quot;, &quot;donations_20&quot;) I’d love your feedback I’d love to get your feedback on this report. (cut …) Ways of leaving feedback. Connection to EA forum post described… 3.1 Introduction and summary Very brief: Statement about the importance of charitable giving and the data collection here. (cut …), what we present, … linked outline here, e.g., … the total magnitude of EA giving and its relationship to non-EA giving, … EA’s donation plans versus realized donations (and future plans). Our modeling work work considers how donations (total, share-of-income, and ‘donated over 1000 USD’) jointly relates to a range of characteristics. We present: ‘descriptive’ results: focusing on demographics, employment and careers, and the ‘continuous features’ age, time-in-EA, income, and year of survey. ‘Predictive’ models, allowing the ‘machine learning’ models themselves to choose which features seem to be most important for predicting donations. In the narrative below, we simply refer to “donations” rather than “reported donations” for brevity. Unless otherwise mentioned, all figures simply add, average, or otherwise summarize individual responses from the EA Survey years mentioned. # Construct charity-specific aggregations # TODO: do for eas_all, move to build side eas_new &lt;- eas_new %&gt;% mutate( num_named_dons = rowSums(!is.na(select(., one_of(all_chars)))), dev_don = rowSums(across(all_of(dev_health_chars)), na.rm = TRUE), d_dev_don = dev_don &gt; 0, animal_don = rowSums(across(all_of(animal_chars)), na.rm = TRUE), d_animal_don = animal_don&gt;0, ea_meta_don = rowSums(across(all_of(ea_meta_chars)), na.rm = TRUE), d_ea_meta_don = ea_meta_don&gt;0, lt_ai_don = rowSums(across(all_of(lt_ai_chars)), na.rm = TRUE), d_lt_ai_don = lt_ai_don&gt;0, other_don = rowSums(across(all_of(other_chars)), na.rm = TRUE), d_other_don = other_don&gt;0 ) %&gt;% mutate_at(.vars =where_don_vars, funs(ifelse(num_named_dons==0, NA, .)) ) eas_new %&lt;&gt;% labelled::set_variable_labels(.labels = as.list(all_char_labels), .strict=FALSE) pct_tot &lt;- function(x, df=eas_new) { x/NROW(df)*100 } num_don &lt;- sum(eas_new$donation_c&gt;0, na.rm=TRUE) num_na_don &lt;- sum(is.na(eas_new$donation_c)) zero_don &lt;- sum(eas_new$donation_c==0, na.rm=TRUE) tot_don &lt;- sum(eas_new$donation_c, na.rm=TRUE) #for all years, for USA nonstudents only tot_don_all_usa &lt;- sum(eas_all$donation_c[eas_all$d_live_usa==1 &amp; eas_all$d_student==0], na.rm=TRUE) tot_inc_all_usa &lt;- sum(eas_all$income_c_imp_bc5k[eas_all$d_live_usa==1 &amp; eas_all$d_student==0], na.rm=TRUE) tot_share_don_us_nonstudent &lt;- tot_don_all_usa/tot_inc_all_usa tot_don_dev &lt;- sum(eas_new$dev_don, na.rm=TRUE) tot_don_animal &lt;- sum(eas_new$animal_don, na.rm=TRUE) tot_don_ea_meta &lt;- sum(eas_new$ea_meta_don, na.rm=TRUE) tot_don_lt_ai &lt;- sum(eas_new$lt_ai_don, na.rm=TRUE) med_don &lt;- median(eas_new$donation_c, na.rm=TRUE) mean_don &lt;- mean(eas_new$donation_c, na.rm=TRUE) mean_don_not_new &lt;- mean(eas_new$donation_c[eas_new$year_involved_n!=year_s], na.rm=TRUE) mean_don_ly &lt;- mean(eas_all$donation_c[eas_all$year==(year_n-1)], na.rm=TRUE) mean_don_ly_not_new &lt;- mean( eas_all$donation_c[eas_all$year==(year_n-1) &amp; as.numeric(eas_all$year_involved)!=(year_n-1)], na.rm=TRUE) plan_donate_ly_c &lt;- filter(eas_all, year == year_n-1) %&gt;% pull(donation_plan_c) mean_plan_ly &lt;- mean(plan_donate_ly_c, na.rm=TRUE) #plan in 2018 for 2019 med_plan_ly &lt;- median(plan_donate_ly_c, na.rm=TRUE) med_not_new &lt;- median(eas_new$donation_c[eas_new$year_involved_n!=year_s], na.rm=TRUE) top_1p3don &lt;- eas_new %&gt;% select(donation_c) %&gt;% slice_max(donation_c, prop =.013) %&gt;% sum() top_1p3share &lt;- top_1p3don/tot_don 3.1.1 Summary, key results and numbers Responses, total and average donation 55.5% of EAs in the 2020 survey reported making a charitable donation in 2019 13.7% reported making zero donations, and 30.8% did not respond to this question. Thus, of those who responded, 80.3% reported making a donation in the prior year. Participants reported total donations of 10,695,926 USD in 2019 (cf 16.1M USD in 2018). Trends The number of survey participants went from 0 in 2019 (0 of whom answered the donation question) to 12841 (7288 answering the donation question) in 2020. Over the past years, we see … (trend in median or mean donation amounts reported).3 Median annual donation in 2019: 528 USD Cf prior year … Mean (reported) annual donation for 2019 was 7,516 USD (cf 9,370 for 2018) or 8,607 USD excluding those who joined in 2020 (cf 10,246 USD for 2018 excluding those who joined in 2019). Median annual donation in 2019 excluding those who joined EA in 2020 was 761 USD (cf. 990 USD for the comparable median for 2018/2019 and 832 USD for 2017/2018). (See ’donation and income trends in EA’ for more details). Donation as shares of income, distribution of donations In 2019 1.3% of donors accounted for $6,437,404 in donations or 60% of the survey total. (Cf …) med_don_share &lt;- median(eas_new$don_share_inc_imp, na.rm = TRUE) med_don_share_imp_bc &lt;- median(eas_new$don_share_inc_imp_bc5k, na.rm = TRUE) med_don_share_ly &lt;- median(eas_lastyr$don_share_inc_imp, na.rm = TRUE) med_don_share_imp_bc_ly &lt;- median(eas_lastyr$don_share_inc_imp_bc5k, na.rm = TRUE) earn_filter &lt;- quos(d_student==0, income_c&gt;10000) med_don_share_imp_ns_10k &lt;- eas_new %&gt;% filter(!!!earn_filter) %&gt;% summarise(med=median(don_share_inc_imp, na.rm = TRUE)) tot_inc &lt;- sum(eas_new$income_c, na.rm=TRUE) tot_inc_imp_bc &lt;- sum(eas_new$income_c_imp_bc5k, na.rm=TRUE) share_don_gt_10pct &lt;- sum(eas_new$don_share_inc_imp&gt;=.1, na.rm = TRUE)/sum(!is.na(eas_new$don_share_inc_imp)) share_don_gt_10pct_imp &lt;- sum(eas_new$don_share_inc_imp_bc5k&gt;=.1, na.rm = TRUE)/sum(!is.na(eas_new$don_share_inc_imp_bc5k)) share_don_gt_5pct_imp &lt;- sum(eas_new$don_share_inc_imp_bc5k&gt;=.05, na.rm = TRUE)/sum(!is.na(eas_new$don_share_inc_imp_bc5k)) share_don_gt_10pct_earn &lt;- eas_new %&gt;% filter(!!!earn_filter) %&gt;% transmute(share_don_gt_10pct = sum(don_share_inc_imp&gt;=.1, na.rm = TRUE)/sum(!is.na(don_share_inc_imp)) ) %&gt;% unlist %&gt;% .[1] #don gt 10pct ... by gender #eas_new %&gt;% # mutate(d_don_gte10_imp = don_share_inc_imp&gt;=.1) %&gt;% # tabyl(gender_manual, d_don_gte10_imp) %&gt;% tabylstuff() The median percentage of income donated in 2019 was 2.14% (cf 2.61% in 2018). However, if we impute “0 and missing incomes” at “group medians for student-status and country”,4 the median percentage of income donated was 2% for 2019. Mean share of total (imputed) income donated was 9.4% (imputing income where below 5k or missing) or 12.5% without imputation. 19.9% of EAs who answered the donation question reported donating 10% or more of their income in 2019 (if we impute income; otherwise 21.4% without imputation; this compares to …, without imputation). The median percent of income donated by full-time-employed non-students who earned more than $10,000 was 2.92%, and of this group 23.9% donated 10% of their income or more in 2018 (cf 3.38% and 24% in 2018). Overall, those taking the EA survey tend to report donating a substantially greater share of income than those in the general US population – (web link). don_tot_freq &lt;- eas_all %&gt;% dplyr::filter(year==max(year)) %&gt;% summarise(across(c(all_of(donate_charity_names)), ~sum(as.numeric(.x) &gt; 0, na.rm = TRUE))) %&gt;% slice(1) %&gt;% unlist(., use.names=TRUE) pct_don &lt;- function(x) { sum(don_tot_freq[x])/sum(don_tot_freq)*100 } pct_ddon &lt;- function(x) { op( sum(x != 0, na.rm=TRUE)/sum(notNA(x), na.rm=TRUE)*100 ) } don_stats &lt;- eas_new %&gt;% filter(num_named_dons&gt;0) %&gt;% select(all_of(where_don_vars)) %&gt;% vtable::sumtable( summ=c(&#39;notNA(x)&#39;, &#39;sum(x != 0)&#39;, &#39;sum(x != 0)/notNA(x)&#39;, &#39;mean(x)&#39;, &#39;sd(x)&#39;, &#39;pctile(x)[50]&#39;, &#39;pctile(x)[90]&#39;), summ.names = c(&#39;Number of Responses&#39;, &#39;Number reporting donation to cause&#39;, &#39;Share of reporters donating to cause&#39;, &quot;Mean donation of reporters (including 0&#39;s)&quot;, &#39;Sd&#39;, &quot;Median&quot;, &quot;90th pct&quot;), digits=c(0,0,2,0,0,0,0), simple.kable = TRUE, labels = all_char_labels2, #it&#39;s a horrible workaround but we need to have the order of these the same as the table order ... I think it&#39;s a flaw of sumtable title = &quot;Donations by category (where indicated)&quot;, out=&quot;kable&quot;) %&gt;% kable_styling() #todo (low-priority) -- replace with .summ hijacked command n_rep_char &lt;- sum(eas_new$num_named_dons&gt;0, na.rm=TRUE) don_stats_by_gwwc &lt;- eas_new %&gt;% mutate(`GWWC Pledge` = case_when( action_gwwc==1 ~ &quot;Yes&quot;, action_gwwc==0 ~ &quot;No&quot; )) %&gt;% filter(num_named_dons&gt;0) %&gt;% select(all_of(where_don_vars), `GWWC Pledge`) %&gt;% vtable::sumtable(group = &quot;GWWC Pledge&quot;, group.test=TRUE, summ=c(&#39;notNA(x)&#39;,&#39;sum(x != 0)/notNA(x)&#39;, &#39;mean(x)&#39;, &#39;sqrt(var(x)/length(x))&#39;, &#39;pctile(x)[50]&#39;), summ.names = c(&#39;N Responses&#39;, &#39;Share positive&#39;, &#39;Mean&#39;, &quot;Median&quot;), digits=c(0,2, 0,0,0), simple.kable = TRUE, labels = all_char_labels2, #it&#39;s a horrible workaround but we need to have the order of these the same as the table order ... I think it&#39;s a flaw of sumtable title = &quot;Donations by category (where indicated), by GWWC&quot;, out=&quot;kable&quot;) %&gt;% row_spec(1:1, bold = TRUE) %&gt;% kable_styling() ddon_stats_by_gwwc &lt;- eas_new %&gt;% mutate(`GWWC Pledge` = case_when( action_gwwc==1 ~ &quot;Yes&quot;, action_gwwc==0 ~ &quot;No&quot; )) %&gt;% filter(num_named_dons&gt;0) %&gt;% select(all_of(where_don_dummies), `GWWC Pledge`) %&gt;% vtable::sumtable(group = &quot;GWWC Pledge&quot;, group.test=TRUE, summ=c(&#39;notNA(x)&#39;,&#39;sum(x != 0)/notNA(x)&#39;), summ.names = c(&#39;N Responses&#39;, &#39;Donated to... ?&#39;), digits=c(0,2), simple.kable = TRUE, labels = all_char_labels2, #it&#39;s a horrible workaround but we need to have the order of these the same as the table order ... I think it&#39;s a flaw of sumtable title = &quot;Binary: Indicated donating to category, by GWWC&quot;, out=&quot;kable&quot;) %&gt;% row_spec(1:1, bold = TRUE) %&gt;% kable_styling() # .kable() %&gt;% # .kable_styling(&quot;striped&quot;) #todo (low-priority) -- replace with .summ hijacked command Where EAs donated While 69.2% of respondents answered the donation question, only 20.9% answered at least one question about where they donated. Among these, the charity that the most EAs stated that they donated to was (…) with 121 reported donations (out of a total of 715 reported donations). Global Poverty charities continue to attract (…) the largest counts and amounts of donations. 62% of those who answered the relevant question reported donating to this category. 55.1% of the total ‘where donated’ reports were to global poverty charities. We sum 1,703,870 USD in total donations reported as specifically going to global poverty charities. This compares to 27.3% reporting donating, 21.4% of donations and \\(\\$\\) 645,086 total donated for animal charities, 17.2%, 11.9% and \\(\\$\\) 330,910 for EA movement/meta charities, and 18.2%, 11.5% and \\(\\$\\) 418,403 for long term and AI charities, respectively. Donations vs plans … Evidence is mixed on whether EAs’ donations in a year tend to exceed or fall short of the amount they planned to donate (as they reported in previous surveys). Descriptive and predictive models Our descriptive models suggest that: … Our predictive (ML) models highlight the importance of … … model performance, brief note about technical modeling choice changes if any Link: Why does the EA Survey ask about donations? See discussion in 2020 EA Forum post. 3.2 Total EA donations, magnitudes in context Considering the magnitude of the donations… The $10,695,926 USD in donations reported above seems likely to be a small share of total EA-affiliated giving, … See our fairly lengthy discussion and benchmarking of this in the 2020 EA Forum post … 3.3 Career paths: Earning-to-give [Caveat about changes in question text (mention and link …)] Changes for students and nonstudents (bearing in mind the above caveats): etg_rates_all &lt;- eas_all %&gt;% filter(year&gt;2014) %&gt;% group_by(year) %&gt;% summarise( &quot;Count&quot; = n(), &quot;Share ETG&quot; = mean(as.numeric(d_career_etg)) ) etg_rates_ns &lt;- eas_all %&gt;% filter(year&gt;2014) %&gt;% filter(d_student==0) %&gt;% group_by(year) %&gt;% summarise( &quot;Count&quot; = n(), &quot;Share ETG&quot; = mean(as.numeric(d_career_etg)) ) ( etg_rates_tab &lt;- bind_cols(etg_rates_all, etg_rates_ns[-1]) %&gt;% magrittr::set_names(c(&quot;Year&quot;, &quot;All responses&quot;, &quot;Share EtG&quot;, &quot;Nonstudents&quot;, &quot;Nonstudents: Share EtG&quot;)) %&gt;% kable(caption = &quot;Rates of &#39;Earning-to-give&#39; by year and student status (see caveats)&quot;, digits=3) %&gt;% .kable_styling() ) Table 3.1: Rates of ‘Earning-to-give’ by year and student status (see caveats) Year All responses Share EtG Nonstudents Nonstudents: Share EtG 2015 2362 0.217 980 0.322 2017 1845 0.220 671 0.380 2018 2599 0.303 1791 0.336 2019 2509 0.283 1898 0.262 2020 2056 0.151 1035 0.232 #todo - medium priority: combine the above tables into a single table: overall, just for students with just n, (etg_rates_plot &lt;- eas_all %&gt;% group_by(year, d_student) %&gt;% filter(year&gt;2014) %&gt;% filter(!is.na(d_student)) %&gt;% #@oska (low-med priority todo): we should functionalize these mutations for computing se and CIs (or find someone who has done). We do it again and again, and the code is bulky #maybe incorporate my se_bin function #@oska todo ... also functionalize or otherwise preserve a good version of this graph # Calculate standard error, confidence bands and change student factor levels summarise( m_etg = mean(as.numeric(d_career_etg)), se = se_bin(d_career_etg)) %&gt;% mutate( etg_low = m_etg - 1.96*se, etg_high = m_etg + 1.96*se, d_student = as.factor(if_else(d_student == 0, &quot;Non-student&quot;, &quot;Student&quot;)), year = as.factor(year)) %&gt;% ggplot(aes(x=year, y=m_etg, colour = d_student, group = d_student)) + geom_pointrange(aes(ymin = etg_low, ymax = etg_high), position = position_dodge(width=0.5)) + # Ensure that bars don&#39;t overlap geom_line(position = position_dodge(width=0.5)) + xlab(&quot;Mean (and 95% CI) response share in &#39;Earning-to-give&#39;&quot;) + ylab(&quot;Share of sample&quot;) + scale_color_discrete(&quot;&quot;) + # Remove legend title scale_y_continuous(labels = scales::percent_format(accuracy = 1L), limits=c(0,NA), oob = scales::squish) + # Change y-axis to percentages theme(legend.position = c(0.9, 0.95), #legend.background = element_rect(fill=alpha(&#39;blue&#39;, 0.001)), legend.key = element_blank()) ) The decline in ETG is less dramatic among non-students (over 23% of non-student respondents still report ETG as their ‘current career’), but it nonetheless appears to be fairly strong and consistent from 2017-present.5 3.4 Donation totals: descriptives 6 Overall donations, totals by groups Below, we present a histogram of positive reported 2018 donations by all respondents. Note that: the horizontal axis is on a logarithmic scale, 13.7% of the 2,056 total respondents reported donating zero, and 30.8% of the total respondents did not report their donation amount. donation_c &lt;- eas_new$donation_c require(scales) don_breaks &lt;- c(50, 100, 200, 300, 500, 1000, 2500, 5000, 10000, 25000, 50000, 100000, 250000, 500000, 1000000, 2500000) eas_new %&lt;&gt;% rowwise() %&gt;% mutate(donation_c_50 = max(donation_c, 50)) %&gt;% ungroup ( donhist_tyly &lt;- eas_new %&gt;% hist_plot_lscale(eas_new$donation_c_50, breaks = don_breaks ) + geom_vline_mean(donation_c) + geom_vline_med(donation_c) + geom_vline_90(donation_c) + labs(title=&quot;Histogram of prior year&#39;s Donations (latest survey)&quot;, x=&quot;last year&#39;s $ Donations (bottom-coded at 50)&quot;, y = &quot;Number of respondents&quot;) ) # Todo (medium importance): Overlay a display of &#39;overall percentage shares&#39; ... so we know where the 80th and 90th percentile are, etc. In 2018 we reported [link/quote] (Or make this into a concise bulleted comparison) … We compare the results for 2020 (for 2019 donations): Median donation (of those reporting): 528 USD Donation of $1000 puts you in the 59.5th percentile. - Being in the top 10% requires donating 9,972 Being in the top 1% means donating 89,560 USD. … comments (or leave out) require(treemapify) geom_treemap_opts &lt;- list(treemapify::geom_treemap(alpha = 0.7), geom_treemap_text(fontface = &quot;italic&quot;, colour = &quot;white&quot;, place = &quot;centre&quot;, grow = TRUE, min.size = 1 ), theme(legend.position = &quot;none&quot;, plot.title = element_text(hjust = 0.5)) ) ( don_share_by_size &lt;- eas_new %&gt;% geom_tree_tot_by_split(donation_c, donation_c_split, title = &quot;Share of total last year&#39;s donation amount, by donation size&quot;) ) … discussion or cut ‘Which career paths are driving donation totals?’ Some caveats Caveats: The figures below exclude 486 participants who provided no response to the career question (… doublecheck this), 0.236 of the sample. These participants reported a total of $$2,766,310 in donations which makes up 25.9% of the total reported donations for 2019 . Totals Overall shares #library(treemapify) don_by_career_XXX &lt;- eas_new %&gt;% geom_tree_tot_by_split(donation_c, career_, title= &quot;Share of last year&#39;s donations by career path&quot;) ( don_by_career &lt;- eas_new %&gt;% select(career_, donation_c) %&gt;% filter(!is.na(career_)) %&gt;% group_by(career_) %&gt;% filter(!is.na(career_)) %&gt;% summarise(total_don = sum(donation_c, na.rm=TRUE), n = n()) %&gt;% mutate(don_share = round(total_don/sum(total_don)*100), freq = n/sum(!is.na(eas_new$career_)) ) %&gt;% ggplot(aes(area = total_don , fill=freq, # Include percentage of total donation label = paste(career_, paste0(don_share, &quot;%&quot;), paste0(&quot;(Pop:&quot;, round(freq*100) , &quot;%)&quot;), sep = &quot;\\n&quot;))) + geom_treemap_opts + # theme(legend.position = &quot;bottom&quot;) + #todo -- add title to legend explaining that it&#39;s the survey pop; get better colors for this scale_fill_continuous(name = &quot;Frequency&quot;, label = scales::percent, trans = &quot;reverse&quot;) + labs(title= &quot;Share of last year&#39;s donations by career path&quot;, subtitle = &quot;(Share of survey population in parentheses; darker = larger share)&quot;) ) ( career_tab &lt;- eas_new %&gt;% mutate(Career = na_if(career_, &quot;na&quot;)) %&gt;% filter(!is.na(Career)) %&gt;% tabyl_ow(Career) %&gt;% adorn_totals() %&gt;% rename_with(snakecase::to_title_case) %&gt;% .kable(caption = &quot;Shares in each career path&quot;, padding = 0, digits=c(0,0,2)) %&gt;% .kable_styling() ) Table 3.2: Shares in each career path Career N Percent Academia 242 15.41 For Profit Earn to Give 237 15.10 Building Capital 203 12.93 For Profit 183 11.66 Still Deciding 167 10.64 Other 166 10.57 Non Profit Ea 130 8.28 Government 96 6.11 Think Tank Lobby 90 5.73 Non Profit 56 3.57 Total 1,570 100.00 We put this in perspective, considering that income levels are different between these career paths: grp_sum &lt;- function(df, xvar, yvar, groupvar) { df %&gt;% dplyr::select({{xvar}}, {{yvar}}, {{groupvar}}) %&gt;% group_by({{groupvar}}) %&gt;% drop_na({{xvar}}, {{yvar}}, {{groupvar}}) %&gt;% summarise( mn_y = mean({{yvar}}), mn_x = mean({{xvar}}), med_y = median({{yvar}}), med_x = median({{xvar}}), se_y = sd({{yvar}}, na.rm=TRUE)/sqrt(length({{yvar}})), se_x = sd({{xvar}}, na.rm=TRUE)/sqrt(length({{xvar}})) ) %&gt;% group_by({{groupvar}}) %&gt;% # Calculate confidence intervals mutate( lower = max(0, mn_y - 1.96*se_y), upper = mn_y + 1.96*se_y ) } plot_grp &lt;- function(df, groupvar, labsize=4, labangle=90, force = 1, fp = 1, mo=10, bp=1, arrow=NULL) { df %&gt;% ggplot(aes(x=mn_x, y=mn_y, label = {{groupvar}})) + geom_point() + geom_abline(intercept = 0, slope = 0.1, colour=&quot;violetred1&quot;) + geom_smooth(method=lm, alpha=0.7) + geom_errorbar(aes(ymin = lower, ymax = upper), alpha=0.7) + scale_y_continuous( oob = scales::squish) + scale_x_continuous( oob = scales::squish) + ggrepel::geom_text_repel( size = labsize, angle = labangle, max.overlaps=mo, force=1, force_pull = fp, box.padding = bp, arrow = arrow, color=&quot;brown&quot;, alpha=0.75) } ( don_inc_career_plot &lt;- eas_new %&gt;% mutate(Career = na_if(career_, &quot;na&quot;)) %&gt;% filter(!is.na(Career)) %&gt;% grp_sum(income_c_imp_bc5k, donation_c, Career) %&gt;% plot_grp(Career, labsize=3) + xlab(&quot;Mean income in USD (imputed if &lt;5k/missing)&quot;) + ylab(&quot;Mean donations, CIs&quot;) + scale_y_continuous(limits=c(-10000, 30000), oob = scales::squish) ) The plot above depicts mean income and mean donations by ‘career group’, with 95% CI’s for the latter. We superimpose a ‘line of best fit’ (blue, with smoothed 95% intervals for this rough fit) and a ‘10% of income donation’ line (red). Unsurprisingly, for-profit ‘not-EtG’ are below the fitted line, and ‘for-profit EtG’ above this line, although 95% CIs are fairly wide. We also note that among people in non-profit careers, there are similar average incomes whether or not the non-profit is EA-aligned, but the non-profit EA people seem to donate somewhat more (although the CI’s do overlap). Next, we present reported donation amounts by income groupings.7 #p_load(treemapify) ( don_share_by_income &lt;- eas_new %&gt;% select(donation_c, income_c_imp_bc_k, income_c_imp_split) %&gt;% filter(!is.na(income_c_imp_bc_k)) %&gt;% group_by(income_c_imp_split) %&gt;% summarise(total_don = sum(donation_c, na.rm=TRUE), n = n()) %&gt;% mutate(don_share = round(total_don/sum(total_don)*100), freq = n/sum(!is.na(eas_new$income_c_imp_split))) %&gt;% ggplot(aes(area = total_don, fill= freq, # Include percentage of total donation label = paste(income_c_imp_split, paste0(don_share, &quot;%&quot;), paste0(&quot;(Pop:&quot;, (round(freq*100, 1)) , &quot;%)&quot;), sep = &quot;\\n&quot;))) + geom_treemap_opts + scale_fill_continuous(name = &quot;Frequency&quot;, label = scales::percent, trans = &quot;reverse&quot;) + labs(title= &quot;Share of last year&#39;s donations by income groups&quot;, subtitle = &quot;(Share of survey population in parentheses; darker = larger share)&quot;) ) earn_tab &lt;- eas_new %&gt;% tabyl_ow_plus(income_c_imp_split) Compare the above graph to the ‘donations by donations size’ graph… Finally, we report donation totals by country. First for 2019 donations alone: #p_load(treemapify) ( don_share_country &lt;- eas_new %&gt;% select(donation_c, country_big) %&gt;% group_by(country_big) %&gt;% summarise(total_don = sum(donation_c, na.rm=TRUE), n = n()) %&gt;% mutate(don_share = round(total_don/sum(total_don)*100), freq = n/sum(!is.na(eas_new$country))) %&gt;% ungroup() %&gt;% filter(don_share != 0 &amp; !is.na(country_big)) %&gt;% ggplot(aes(area = total_don, fill= freq, # Include percentage of total donation label = paste(country_big, paste0(don_share, &quot;%&quot;), paste0(&quot;(Pop:&quot;, op(round(freq*100, 0)) , &quot;%)&quot;), sep = &quot;\\n&quot;))) + geom_treemap_opts + #scale_fill_continuous(name = &quot;Frequency&quot;, label = scales::percent, trans = &quot;reverse&quot;) + scale_fill_continuous(name = &quot;Frequency&quot;, label = scales::percent, trans = &quot;reverse&quot;) + labs(title= &quot;Share of last year&#39;s donations by country&quot;, subtitle = &quot;(Share of survey population in parentheses; darker = larger share)&quot;) ) #; darker = larger share Next, pooling across all years of the EA survey (without any weighting or adjustment): ( don_share_country_all_years &lt;- eas_all %&gt;% select(donation_c, country, year) %&gt;% filter(!is.na(country)) %&gt;% group_by(country) %&gt;% summarise(total_don = sum(donation_c, na.rm=TRUE), n = n()) %&gt;% ungroup() %&gt;% mutate(don_share = round(total_don/sum(total_don)*100), freq = n/sum(!is.na(eas_all$country))) %&gt;% filter(don_share &gt; 0.1) %&gt;% mutate(country = snakecase::to_title_case(country)) %&gt;% ggplot(aes(area = total_don, fill= freq, # Include percentage of total donation label = paste(country, paste0(don_share, &quot;%&quot;), paste0(&quot;(Pop:&quot;, op(round(freq*100, 0)) , &quot;%)&quot;), sep = &quot;\\n&quot;))) + geom_treemap_opts + scale_fill_continuous(name = &quot;Frequency&quot;, label = scales::percent, trans = &quot;reverse&quot;) + theme(legend.position = &quot;none&quot;, plot.title = element_text(hjust = 0.5)) + labs(title= &quot;Share of total (all years) donation amounts by country&quot;, subtitle = &quot;(Share of survey population in parentheses; darker = larger share)&quot;) ) And again, ‘Winsorizing’ donations at 100K USD (setting larger donations at this value), to reduce the impact of outliers: ( don_share_country_all_years_w &lt;- eas_all %&gt;% select(donation_c, country, year) %&gt;% filter(!is.na(country)) %&gt;% rowwise() %&gt;% mutate(donation_c_w = min(donation_c, 100000)) %&gt;% ungroup() %&gt;% group_by(country) %&gt;% summarise(total_don_w = sum(donation_c_w, na.rm=TRUE), n = n()) %&gt;% ungroup() %&gt;% mutate(don_share = round(total_don_w/sum(total_don_w)*100), freq = n/sum(!is.na(eas_all$country))) %&gt;% filter(don_share &gt; 0.1) %&gt;% mutate(country = snakecase::to_title_case(country)) %&gt;% ggplot(aes(area = total_don_w, fill= freq, # Include percentage of total donation label = paste(country, paste0(don_share, &quot;%&quot;), paste0(&quot;(Pop:&quot;, op(round(freq*100, 0)) , &quot;%)&quot;), sep = &quot;\\n&quot;))) + geom_treemap_opts + scale_fill_continuous(name = &quot;Frequency&quot;, label = scales::percent, trans = &quot;reverse&quot;) + theme(legend.position = &quot;none&quot;, plot.title = element_text(hjust = 0.5)) + labs(title= &quot;Share of 100k-Winsorised donations by country; all years&quot;, subtitle = &quot;(Share of survey population in parentheses; darker = larger share)&quot;) ) #TODO - @oska -- UK and USA in all-caps above #TODO - @oska -- capitalization below #TODO - @oska -- sort by shares below ( country_tab &lt;- eas_all %&gt;% group_by(country_big) %&gt;% filter(year&gt;2014) %&gt;% mutate( year_2020 = case_when( year==2020 ~ &quot;2019 share.&quot;, TRUE ~ &quot;pre-2019 share.&quot; ), `Country` = str_to_title(country_big), ) %&gt;% tabyl(`Country`, year_2020) %&gt;% adorn_percentages(&quot;col&quot;) %&gt;% .kable(digits=2, caption=&quot;Shares (0-1) of survey population by country; larger countries only&quot;, label=TRUE) %&gt;% .kable_styling() ) Table 3.3: Shares (0-1) of survey population by country; larger countries only Country 2019 share. pre-2019 share. Australia 0.04 0.04 Belgium 0.01 0.00 Brazil 0.00 0.01 Canada 0.03 0.03 Czech Republic 0.02 0.01 Denmark 0.00 0.00 France 0.02 0.01 Germany 0.06 0.05 Netherlands 0.02 0.01 New Zealand 0.02 0.01 Norway 0.02 0.01 Other 0.09 0.06 Poland 0.00 0.00 Sweden 0.01 0.01 Switzerland 0.02 0.01 Uk 0.11 0.12 Usa 0.30 0.29 NA 0.22 0.32 #d_anim &lt;- &quot;Y&quot; #library(gganimate) anim_filename &lt;- here(plots_folder, &quot;animated_tree_plot.gif&quot;) if (exists(&quot;d_anim&quot;)) { if (d_anim == &quot;Y&quot;) { animated_dons_country &lt;- eas_all %&gt;% select(year, donation_c, country_big) %&gt;% group_by(year, country_big) %&gt;% filter(year&gt;2014) %&gt;% summarise(total_don = sum(donation_c, na.rm=TRUE)) %&gt;% mutate(don_share = round(total_don/sum(total_don)*100)) %&gt;% ggplot(aes(area = total_don, fill= country_big, # Include percentage of total donation label = paste(country_big, paste0(don_share, &quot;%&quot;), sep = &quot;\\n&quot;))) + geom_treemap_opts + ggtitle(&quot;Share of total last year&#39;s reported donation amounts by country&quot;) anim &lt;- animated_dons_country + transition_states(year, state_length = 3) + ggtitle(&quot;Share of total {closest_state} reported donation amounts by country&quot;) gganimate::anim_save(anim_filename, anim) anim } else{ knitr::include_graphics(anim_filename) } } if (!exists(&quot;d_anim&quot;)){ knitr::include_graphics(anim_filename) } #Todo (medium importance): slo Summarize or skip discussion of this here…. We return to presenting a ‘controlled descriptive picture’ in our modeling work.8 3.4.1 Donation (shares) vs. income and GWWC pledge Quick notes or recap … (or skip) The histograms beloware first only for those with positive reported incomes, and next with the previously-discussed income imputation. The blue vertical line depicts the share of total (imputed) income donated by all respondents, with the green line depicting the median and the red line the 90th percentile. … scale_x_set &lt;- list(scale_x_continuous(limits=c(0,0.35), n.breaks=20)) ( don_share_inc_imp_hist &lt;- eas_new %&gt;% hist_plot(don_share_inc_imp) + geom_vline_med(eas_new$don_share_inc_imp, tgap=0.01) + geom_vline_mean(tot_don/tot_inc, tgap=0.01, label = &quot;Overall share&quot;) + geom_vline_90(eas_new$don_share_inc_imp, tgap=0.005) + scale_x_set + labs(title=&quot;2019 Donations/Income (no imputing)&quot;, x=&quot;2019 Donations/income&quot;, y=&quot;Number of respondents&quot;) + ylim(0, 300) ) ##Todo -- Medium priority: mean is missing # todo -- low priority: make the above histogram bigger, it&#39;s smaller than the rest don_share_inc_imp_hist_imp &lt;- eas_new %&gt;% hist_plot(don_share_inc_imp_bc5k) + geom_vline_mean(tot_don/tot_inc_imp_bc, tgap=0.01, label = &quot;Overall share&quot;) + geom_vline_med(eas_new$don_share_inc_imp_bc5k, tgap=0.005) + geom_vline_90(eas_new$don_share_inc_imp_bc5k, tgap=0.005) + scale_x_set + labs(title=&quot;2019 Donations/Income (with imputing)&quot;, x=&quot;2019 Donations/income (with imputing)&quot;, y = &quot;Number of respondents&quot;) + ylim(0, 300) don_share_inc_imp_hist_imp #Todo -- Medium priority(@oska): convert to &#39;share of respondents&#39;, add cumulative plot don_share_inc_imp_hist_imp %&gt;% ggplotly() The noticeable spike at 10% likely reflects the GWWC pledge (we return to this further below). As noted above, 19.9% of EAs reported a donation at or above 10% of their (imputed) income in 2019. 36.1% reported an amount at or above 5%. #Donations and donation shares -- scatterplots by income and GWWC &#39;action&#39; p_load(ggpubr) op_ax &lt;- function(x) round(as.numeric(x), digits=2) scale_y_don &lt;- scale_y_log10( name = &quot;Donation amount (bottom-coded at $50)&quot;, # labels = scales::dollar, labels = scales::label_number_si(prefix = &quot;$&quot;), n.breaks = 10, limits = c(50, NA) ) don_income_gwwc_sp &lt;- eas_all %&gt;% filter(year==2020) %&gt;% ggpubr::ggscatter( x = &quot;income_c_imp_bc_k&quot;, y = &quot;donation_c_min50&quot;, color = &quot;d_gwwc_ever&quot;, size = 0.8, xlab = &quot;Income in $1k USD (imputed where missing or lt 5k)&quot;, repel = TRUE, palette = &quot;jco&quot;, yscale = &quot;log10&quot;, xscale = &quot;log10&quot;, add = &quot;loess&quot;, add.params = list(color = &quot;black&quot;, fill = &quot;lightgray&quot;), conf.int = TRUE ) + labs(title = &quot;Donations by income (log scales)&quot;) + scale_x_log10(name=&quot;Income in $1K USD (imputed if &lt;5k/missing)&quot;, labels = op_ax, n.breaks=5, limits=(c(5,5000))) + labs(colour = &quot;Mentioned taking GWWC pledge&quot;) + scale_y_don + theme(axis.text.x = element_text( angle = 90, vjust = 0.5, hjust = 1 )) don_income_gwwc_sp_gwwc &lt;- eas_all %&gt;% filter(year==2020) %&gt;% ggplot(aes(x = income_c_imp_bc_k, y = donation_c_min50, color = d_gwwc_ever)) + geom_point(size = 1, alpha = 0.7) + # draw the points geom_smooth(aes(method = &#39;loess&#39;, fill = d_gwwc_ever)) + # @Oska -- note I am using local smoothing here. scale_x_log10(name = &quot;Income in $1K USD (imputed if below 5k/missing)&quot;, n.breaks = 5, limits = c(5, 5000)) + scale_y_log10( name = &quot;Donation amount (bottom-coded at $50)&quot;, # labels = scales::dollar, labels = scales::label_number_si(prefix = &quot;$&quot;), n.breaks = 10, limits = c(50, NA) ) + scale_color_discrete(name = &quot;GWWC pledge&quot;) + scale_fill_discrete(guide = &quot;none&quot;) + theme(axis.text.x = element_text( angle = 90, vjust = 0.5, hjust = 1 ), legend.position = c(.87,.15), legend.background = element_rect(fill=alpha(&#39;blue&#39;, 0.01))) ##Todo -- Medium priority - clean up the above a bit more... get the axes better so that we can really see the &#39;large mass in the middle a bit better. Maybe slightly smaller dots and bolder smoothed lines, perhaps different colors for the CI shading for each # - perhaps use geom_pointdensity with different shapes to indicate regions of &quot;larger mass&quot; # #TODO -- Add some layer to better capture the masses *exactly at* 10pct # REVIEW # We should note that this doesn&#39;t include those who donate nothing due to the log scale (pseudo log scale is a bit weird here as well) require(ggpointdensity) don_share_income_by_X &lt;- eas_all %&gt;% filter(year==2020) %&gt;% mutate(income_c_imp_bc5k_k = income_c_imp_bc5k/1000) %&gt;% rowwise() %&gt;% mutate(don_share_inc_imp_bc5k = min(don_share_inc_imp_bc5k, 0.4)) %&gt;% ungroup() %&gt;% group_by(d_gwwc_ever_0) %&gt;% mutate(med_gwwc = median(don_share_inc_imp_bc5k, na.rm=TRUE)) %&gt;% ungroup() %&gt;% group_by(engage_high_n) %&gt;% mutate(med_eng = median(don_share_inc_imp_bc5k, na.rm=TRUE)) %&gt;% ggplot(aes(x = income_c_imp_bc5k_k, y = don_share_inc_imp_bc5k)) + ggpointdensity::geom_pointdensity(adjust=0.25) + geom_smooth(method = &quot;loess&quot;) + #geom_hline_med(y) + geom_hline(yintercept=0.1, linetype=&quot;dashed&quot;, size=0.5, color = &quot;red&quot;) + scale_y_continuous(labels = scales::label_percent(accuracy = 1L)) + scale_x_log10(breaks = scales::log_breaks(n=7)) + scale_color_viridis_c(&quot;density of respondents&quot;) + xlab(&quot;Income in $1K USD (imputed if missing, bottom-code at 5k)&quot;) + theme(axis.title.x = element_text(size = 10)) + ylab(&quot;Donations/Income (top-code at 40%)&quot;) don_share_income_by_engage_sp &lt;- don_share_income_by_X + geom_hline(aes(yintercept=med_eng), linetype=&quot;dashed&quot;, size=0.5, color = &quot;blue&quot;) + facet_wrap(~engage_high_n, nrow=3) + ylab(&quot;Donations/Income (top-coded at 50%)&quot;) + labs(title=&quot;By &#39;High-engagement&#39;: last year&#39;s &#39;Don. shares of income&#39; by income (w. imputing)&quot;) don_share_income_by_gwwc_sp &lt;- don_share_income_by_X + geom_hline(aes(yintercept=med_gwwc), linetype=&quot;dashed&quot;, size=0.5, color = &quot;blue&quot;) + facet_wrap(~d_gwwc_ever_0) + labs(title=&quot;By GWWC: last year&#39;s &#39;Don. share of income&#39; by income (w/ imputing)&quot;) How do donations relate to income, and does this relationship differ between those who mention that they took the Giving What We Can (10%) pledge? don_income_gwwc_sp We give a scatterplot of reported donations against income, faceted by GWWC pledge, with separate locally-smoothed conditional means (and 95% confidence intervals for these conditional means). (The figure below is for 2019 donations only.) don_income_gwwc_sp_gwwc … Next we plot donations as shares of income against income for non-GWWC pledgers (combined with non-responders) and GWWC pledgers. The median for each group is given by the dashed blue line, and the dashed red line represents 10 percent of income. don_share_income_by_gwwc_sp … ( tab_don_by_year_pledge &lt;- eas_all %&gt;% filter(!is.na(d_don_10pct_bc5k) &amp; year&gt;=2015) %&gt;% mutate(`Survey year` = year, d_don_plan_10pct = as.numeric(donation_plan_c/income_c_imp_bc5k &gt;=0.1), d_don_plan_10pct = if_else(year&lt;2018, NaN, d_don_plan_10pct)) %&gt;% group_by(d_gwwc_ever_0, `Survey year`) %&gt;% summarise(n = n(), &quot;Donated 10% of income&quot; = mean(d_don_10pct_bc5k), &quot;Donated 10% of income (plan)&quot; = mean(d_don_plan_10pct, na.rm=TRUE) ) %&gt;% rename(&quot;Ever GWWC pledge&quot; = d_gwwc_ever_0) %&gt;% adorn_rounding(digits = 2) %&gt;% kable(caption = &quot;GWWC pledgers: Don. 10%+ of income by survey year (exclusions: see text)&quot;, label=TRUE) %&gt;% .kable_styling() ) Table 3.3: GWWC pledgers: Don. 10%+ of income by survey year (exclusions: see text) Ever GWWC pledge Survey year n Donated 10% of income Donated 10% of income (plan) No/NA 2015 819 0.13 NaN No/NA 2017 673 0.16 NaN No/NA 2018 1221 0.13 0.19 No/NA 2019 1125 0.12 0.16 No/NA 2020 959 0.11 0.19 Yes 2015 352 0.36 NaN Yes 2017 354 0.36 NaN Yes 2018 668 0.40 0.53 Yes 2019 579 0.37 0.48 Yes 2020 463 0.39 0.48 Among those who report having ever taken a GWWC pledge (and who report donations, and excluding those reporting incomes below 5000 USD) [… share who report donating over 10%, discussion and caveats] Further discussion (or link), relation to previous posts and figures, linkk to supplements… 3.4.2 Employment and student status We present income and donation statistics for those “statuses” with more than 50 respondents in the forest plot below (a full table of statistics for each group can be found in the appendix).9 In each of the forest plots in this subsection, the blue line presents a simple linear best-fit of these points, and the red line represents a 10% donation rate. se &lt;- function(x) sqrt(var(x)/length(x)) sumstatvec &lt;- c(&quot;{median}&quot;, &quot;{p10}-{p90}&quot;, &quot;{mean} [{se}] ({sd})&quot;) doninclabs &lt;- list(income_k_c ~ &quot;Income in $1000 USD&quot;, donation_c ~ &quot;Last year&#39;s donation (in USD)&quot;, donation_plan_c ~ &quot;Latest planned donation&quot;) don_inc_by_student &lt;- eas_new %&gt;% group_by(status_) %&gt;% mutate( status_ = as.character(status_), large_group = case_when( n()&lt;50 ~ &quot;Other&quot;, TRUE ~ status_) ) %&gt;% ungroup() %&gt;% dplyr::select(income_k_c, donation_c, donation_plan_c, large_group) %&gt;% tbl_summary(by = large_group, type = c(all_continuous()) ~ &quot;continuous2&quot;, statistic = list(all_continuous() ~ sumstatvec), label = doninclabs, missing = c(&quot;no&quot;) ) %&gt;% bold_labels() %&gt;% add_n() %&gt;% add_overall() #TODO: High -- fix the column labels #todo (low) -- we use this several times and it&#39;s a good format; let&#39;s functionalise it #Todo (medium): Bootstrapping the SE of the median would be nice, see, e.g., https://clayford.github.io/dwir/dwr_12_generating_data.html library(ggrepel) # # 1.summarize donation and income (mean and 95pct CI for each) by status_ # 2. plot median (and mean) donation by income for each group (income lowest to highest) # 3. fit a line/curve of donation by income for each group (do for ) -- replace with the regression line based on the population not the groups # 4. Add error bars (for donations, not income) -- hard to do for median, though #TODO -- High Priority: Make this nice in the ways discussed (@oska it seems you have already started this) # why are the error bars not surrounding the point? # make it pretty (use your judgment), fix labels, add median colored dot, ( don_inc_status_plot &lt;- eas_new %&gt;% mutate( status_ = str_replace_all( status_, c(&quot;_&quot; = &quot; &quot;) ) ) %&gt;% grp_sum(income_c_imp_bc5k, donation_c, status_) %&gt;% plot_grp(status_, labsize=3, fp=0.3, force=5, mo=20, bp=1.5, arrow = arrow(length = unit(0.02, &quot;npc&quot;)) ) + xlab(&quot;Mean income in USD (imputed if &lt; 5k/missing)&quot;) + ylab(&quot;Mean donations, 95% CIs&quot;) + scale_y_continuous(limits=c(-10000, 30000), oob = scales::squish) ) # Todo (low): Plot regression line for full pop # Todo: HIGH -- get this to look nicer, label it better, add better axis breaks (every 5k for donation, every 20k for income) #Todo (Medium) -- add plots for the medians #Todo Donations generally track income by this aggregation, with some groups possibly ‘under-performing’ or ‘over-performing’; we return to this in our descriptive modeling.10 3.4.3 Donations by country Donations and income by country We report similar income and donation statistics for all countries with more than 50 respondents: ( don_income_by_ctry &lt;- eas_new %&gt;% dplyr::select(income_k_c, donation_c, donation_plan_c, country_big) %&gt;% tbl_summary( by = country_big, sort = all_categorical() ~ &quot;frequency&quot;, #reverse this ordering or maybe reverse sort by average income type = c(all_continuous()) ~ &quot;continuous2&quot;, statistic = list(all_continuous() ~ sumstatvec), label = doninclabs, missing = c(&quot;no&quot;) ) %&gt;% bold_labels() %&gt;% add_n() %&gt;% add_overall() ) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #ppxqnpkvxn .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #ppxqnpkvxn .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #ppxqnpkvxn .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #ppxqnpkvxn .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; border-top-color: #FFFFFF; border-top-width: 0; } #ppxqnpkvxn .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ppxqnpkvxn .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #ppxqnpkvxn .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #ppxqnpkvxn .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #ppxqnpkvxn .gt_column_spanner_outer:first-child { padding-left: 0; } #ppxqnpkvxn .gt_column_spanner_outer:last-child { padding-right: 0; } #ppxqnpkvxn .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #ppxqnpkvxn .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #ppxqnpkvxn .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #ppxqnpkvxn .gt_from_md > :first-child { margin-top: 0; } #ppxqnpkvxn .gt_from_md > :last-child { margin-bottom: 0; } #ppxqnpkvxn .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #ppxqnpkvxn .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #ppxqnpkvxn .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #ppxqnpkvxn .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #ppxqnpkvxn .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #ppxqnpkvxn .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #ppxqnpkvxn .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #ppxqnpkvxn .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ppxqnpkvxn .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #ppxqnpkvxn .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #ppxqnpkvxn .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #ppxqnpkvxn .gt_sourcenote { font-size: 90%; padding: 4px; } #ppxqnpkvxn .gt_left { text-align: left; } #ppxqnpkvxn .gt_center { text-align: center; } #ppxqnpkvxn .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #ppxqnpkvxn .gt_font_normal { font-weight: normal; } #ppxqnpkvxn .gt_font_bold { font-weight: bold; } #ppxqnpkvxn .gt_font_italic { font-style: italic; } #ppxqnpkvxn .gt_super { font-size: 65%; } #ppxqnpkvxn .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 65%; } Characteristic N Overall, N = 1,6071 australia, N = 82 belgium, N = 13 brazil, N = 9 canada, N = 59 czech republic, N = 32 denmark, N = 10 france, N = 51 germany, N = 123 netherlands, N = 51 new zealand, N = 37 norway, N = 48 Other, N = 179 poland, N = 10 sweden, N = 16 switzerland, N = 45 uk, N = 221 usa, N = 621 Income in $1000 USD 1,384 Median 33 36 31 7 27 17 17 13 17 15 44 29 18 9 41 25 36 50 10%-90% 1-130 7-109 3-50 4-54 3-86 2-54 9-71 0-42 2-70 0-64 0-120 1-72 0-102 5-24 2-59 0-77 3-82 2-192 Mean [se] (SD) 60 [4] (136) 49 [5] (46) 29 [5] (18) 20 [9] (24) 41 [6] (42) 25 [4] (23) 33 [11] (35) 18 [3] (17) 28 [3] (29) 31 [6] (38) 52 [9] (50) 34 [5] (31) 40 [5] (56) 13 [3] (9) 34 [8] (27) 39 [6] (37) 69 [17] (242) 85 [6] (146) Last year's donation (in USD) 1,397 Median 533 872 237 183 231 100 626 178 355 237 1,026 654 296 119 805 484 660 1,000 10%-90% 0-9,577 0-7,271 0-5,468 0-2,015 0-6,948 0-1,796 46-7,124 0-3,551 0-4,971 0-2,367 14-12,317 0-5,453 0-5,367 4-861 25-5,339 55-12,049 0-6,600 0-16,180 Mean [se] (SD) 7,348 [1,956] (73,113) 3,647 [971] (8,411) 1,301 [679] (2,351) 759 [500] (1,322) 2,753 [930] (6,769) 589 [180] (933) 2,672 [1,230] (3,889) 1,178 [283] (1,919) 1,582 [273] (2,850) 925 [226] (1,536) 5,157 [2,492] (14,315) 2,025 [385] (2,523) 2,041 [402] (4,354) 337 [195] (552) 2,267 [914] (3,167) 16,645 [13,326] (85,328) 20,727 [12,512] (181,320) 7,211 [864] (20,215) Latest planned donation 1,377 Median 1,000 1,636 473 733 761 398 528 473 947 395 1,369 1,091 414 172 1,769 548 1,320 2,000 10%-90% 0-12,020 11-12,360 44-5,848 165-2,840 0-8,676 0-4,030 0-6,054 0-4,660 17-5,823 0-4,438 192-6,843 109-6,380 0-7,640 55-1,399 12-13,002 55-12,049 0-7,788 0-21,000 Mean [se] (SD) 9,831 [2,001] (74,249) 4,852 [964] (8,404) 2,115 [960] (3,461) 1,165 [501] (1,325) 2,909 [844] (6,142) 1,320 [459] (2,342) 1,716 [876] (2,628) 2,026 [514] (3,450) 2,216 [360] (3,692) 1,358 [280] (1,901) 5,233 [2,465] (14,162) 2,434 [439] (2,915) 3,996 [1,287] (13,798) 630 [406] (1,149) 6,071 [3,311] (11,468) 4,522 [1,618] (10,358) 24,592 [11,797] (169,316) 12,033 [2,345] (54,382) 1 c(&quot;Median&quot;, &quot;10%-90%&quot;, &quot;Mean [se] (SD)&quot;) #todo (medium?): make a stem-leaf thing here #todo (High): add *medians* to the above # don_inc_status_plot &lt;- eas_new %&gt;% # dplyr::select(status_, donation_c, income_k_c) %&gt;% # group_by(status_) %&gt;% # drop_na(status_, donation_c, income_k_c) %&gt;% # summarise(across(c(donation_c, income_k_c), # list(mean=mean, # median=median, # se = ~sd(.x)/sqrt(length(.x))))) %&gt;% # group_by(status_) %&gt;% p_load(ggimage) country_codes &lt;- tibble(country = c(&quot;Australia&quot;, &quot;Canada&quot;, &quot;France&quot;, &quot;Germany&quot;, &quot;Netherlands&quot;, &quot;Other&quot;, &quot;United Kingdom&quot;, &quot;USA&quot;), code = c(&quot;ac&quot;, &quot;ca&quot;, &quot;fr&quot;, &quot;de&quot;, &quot;nl&quot;, &quot;yt&quot;, &quot;gb&quot;, &quot;us&quot;)) ( don_inc_country_plot &lt;- eas_new %&gt;% grp_sum(income_c_imp_bc5k, donation_c, country_big) %&gt;% left_join(., country_codes, by = c(&quot;country_big&quot; = &quot;country&quot;)) %&gt;% plot_grp(country_big) + xlab(&quot;Mean income in USD (imputed if &lt;5k/missing)&quot;) + ylab(&quot;Mean donations, CIs&quot;) + scale_y_continuous(limits=c(-3000, 30000), oob = scales::squish) ) #Note -- plotly seems to destroy country labels here #+ggimage::geom_flag() Above, we plot donations and income by country of residence for the countries with the largest number of EA respondents. We fit a simple best-fit (least-squares) line in blue, and add a red line depicting a 10% donation rate. Again, donations generally track income, with some under and over-performers (see later modeling).11 ( don_by_country_viol_ly &lt;- eas_new %&gt;% plot_box_pt_viol(donation_c, country_big, notch=TRUE) + labs(title = &quot;Donation amounts by country (last year)&quot;) ) ( don_by_country_viol_all &lt;- eas_all %&gt;% plot_box_pt_viol(donation_c, where_live_cat, notch=TRUE) + labs(title = &quot;Donation amounts by country grouping (2013-2019)&quot;) ) ( don_by_yr_viol_all &lt;- eas_all %&gt;% mutate(year=as.factor(year)) %&gt;% plot_box_pt_viol(donation_c, year) + labs(title = &quot;Donation amounts by year&quot;) ) Donations, age and years in EA Next, we consider how donations may increase or decrease with ‘time-in-EA’ (i.e., ‘tenure’) as well as age.12 Note to MOSS13 don_by_tenure_facet_age &lt;- eas_new %&gt;% filter(!is.na(age_ranges)) %&gt;% ggplot() + aes(x = tenure, y = donation_c_min50) + geom_point(size = 0.15, colour = &quot;#0c4c8a&quot;, position = position_jitter(seed = 42, width = 0.1, height = 0.001)) + geom_smooth(span = 0.75) + scatter_theme + facet_grid(vars(), vars(age_ranges), scales = &quot;free&quot;) + labs(title = &quot;Last year&#39;s donation by time in EA&quot;, subtitle = &quot;Faceted by Age ranges&quot;) + labs(x = get_label(eas_new$tenure)) + scale_y_don don_by_tenure_facet_age %&gt;% ggplotly() We next report the comparable chart for donation as a share of income: donshare_by_tenure_facet_age &lt;- eas_new %&gt;% filter(!is.na(age_approx_ranges)) %&gt;% ggplot() + aes(x = tenure, y = don_share_inc_imp_bc5k) + geom_point(size = 0.15, colour = &quot;#0c4c8a&quot;, position = position_jitter(seed = 42, width = 0.1, height = 0.001)) + geom_smooth(span = 0.75) + scatter_theme + facet_grid(vars(), vars(age_approx_ranges), scales = &quot;free&quot;) + labs(title = &quot;Last year&#39;s donation as share of (imputed) income by time in EA&quot;, subtitle = &quot;Faceted by Age ranges&quot;) + labs(x = get_label(eas_new$tenure)) + ylab(element_blank()) + ylim(0, 0.3) donshare_by_tenure_facet_age %&gt;% ggplotly [Discussion or link, mention and link analysis ‘within-referrer’… ] By referrer Next, we consider how donations vary by ‘which referrer’ (i.e., which link) took an individual to the EA survey. Again, the blue line gives linear fit (for group means), and the red line the slope for donating 10% of income. ( don_inc_referrer_plot &lt;- eas_new %&gt;% grp_sum(income_c_imp_bc5k, donation_c, referrer_cat) %&gt;% plot_grp(referrer_cat) + scale_y_continuous(limits=c(0, 15000), oob = scales::squish) + xlab(&quot;Mean income by group in USD (imputed if &lt;5k/missing)&quot;) + ylab(&quot;Mean donations by group, CIs&quot;) + ggtitle(&quot;Donation by income and referrer&quot;) ) # (Todo?) I wonder if we should get rid of the blue line and gray line for this … or replace it with one from an individual-based regression 3.5 Donation and income for recent years Methodological note: changing EAS representation As the composition of EAS responses change from year to year, it may be difficult to tell whether EAs (as individual or in total) have been donating more or less in recent years. We discuss this further in the online bookdown supplement chapter. Because of these limitations: we report both totals and averages below, we advise caution in interpreting the amounts and changes we return to this in a controlled model, which is also subject to similar limitations, we defer more detailed analysis of this question for future work. The plot and tests below depict and consider the year-to-year changes in reported donations (subject to caveats noted in the fold above).14 We first consider donation rates in each year for those who answer the donation question (reporting 0 or positive amounts). library(ggstatsplot) library(pairwiseComparisons) ( don_by_year_tab &lt;- eas_all %&gt;% filter(year&gt;=2015) %&gt;% sumtab(donation_c, year, caption=&quot;Donations by year&quot;, digits=c(0, 0, 2, 0, 0, 0,0)) %&gt;% kable_styling() #redundant but helps with parsing ) Table 3.3: Donations by year year N share &gt; 0 Mean Median P80 Std.dev. 2015 1171 0.76 5775 333 3759 (39271.5) 2017 1028 0.84 9505 657 5790 (75452.24) 2018 1889 0.85 9763 740 5201 2019 1704 0.82 9370 684 5000 (87598.4) 2020 1423 0.80 7516 528 4347 (72810.66) We next consider a similar report with donation-nonresponses coded as 0. Discussion/details The above only considers people who did answer donation questions. At an extreme we could consider all non-responses as reflecting people who made (little or) no donations, for a lower bound on on donation rates. As a compromise measure, probably a tighter lower bound, we might assume that people willing to report their incomes are generally willing to answer financial questions. Thus if they do not report their donations it seems more reasonable to suspect that they did not donate in a big way. We thus consider the subset of the above who reported their income, considering similar statistics as above for a modified donation variable, coded as ‘0’ where the donation was not reported. ( don0_by_year_tab &lt;- eas_all %&gt;% filter(!is.na(income_c) &amp; year&gt;2014) %&gt;% #rowwise() %&gt;% #mutate(donation_c_0 = if_else(is.na(donation_c), 0, donation_c)) %&gt;% #ungroup() %&gt;% sumtab(donation_c_0, year, caption=&quot;Donations by year for those reporting income (missings coded as 0)&quot;, digits=c(0, 0, 2, 0, 0, 0, -1, 0)) %&gt;% kable_styling() ) Table 3.3: Donations by year for those reporting income (missings coded as 0) year N share &gt; 0 Mean Median P80 Std.dev. 2015 1033 0.69 4746 250 3094 (35095.05) 2017 1008 0.82 8905 615 5118 (75243.94) 2018 1835 0.83 9952 740 5201 (139809.56) 2019 1682 0.82 9443 684 5000 (88165.41) 2020 1409 0.80 7452 528 4098 (73109.73) #todo same for GWWC people (member_gwwc needs reconciling) #todo -- include a &#39;total donations row&#39;. maybe plot/graph this stuff; Graphs:donation densities ( don_by_year_viol_test &lt;- eas_all %&gt;% #select(donation_c_min50, year) %&gt;% #select(donation_c, year) %&gt;% mutate(year = year-1) %&gt;% filter(year&gt;2014) %&gt;% ggbetweenstats(y = donation_c, x = year, ylab = &quot;Donations (USD)&quot;, # plot.type = &quot;violin&quot;, # type of plot type=&quot;parametric&quot;, conf.level = 0.95, # pairwise.display = &quot;significant&quot;, #p.adjust.method = &quot;hol&quot;, #results.subtitle = &quot;false&quot;, title = &quot;Donations by year, 2016-Present&quot; ) + theme(legend.position=&quot;none&quot;) + ylim(50, NA) + scale_y_continuous(trans = &quot;pseudo_log&quot;, breaks = don_breaks, labels = scales::dollar_format()) ) ( don_by_year_viol_test_ldon &lt;- eas_all %&gt;% #select(donation_c_min50, year) %&gt;% #select(donation_c, year) %&gt;% mutate(year = year-1, ldon1 = log(donation_c+1)) %&gt;% filter(year&gt;2014) %&gt;% ggbetweenstats(y = ldon1, x = year, ylab = &quot;Log (Donations +1)&quot;, # plot.type = &quot;violin&quot;, # type of plot type=&quot;parametric&quot;, conf.level = 0.95, # pairwise.display = &quot;significant&quot;, #p.adjust.method = &quot;hol&quot;, #results.subtitle = &quot;false&quot;, title = &quot;(Log) donations by donation year, 2016-Last year&quot; ) + theme(legend.position=&quot;none&quot;) ) #%&gt;% ggplotly() #Below: replaced this with &#39;wilcox, the nonparametric test&#39; ... but note that is on top of the tests given by the ggbetweenstats command 3.6 Which charities (causes and categories) are EAs donating to? Only a small share of respondents report where they are donating. We group this into several categories summarized below, reporting for only those 429 respondents who indicated at least one category of donations. #don-statistics-category-show, #TODO - HIGH: add better cause labels to this, visualise it in a way that conveys the aggregate shares of donations counts and amounts #created near the top of this file don_stats (#tab:don_stats)Donations by category (where indicated) Variable Number of Responses Number reporting donation to cause Share of reporters donating to cause Mean donation of reporters (including 0’s) Sd Median 90th pct Global health + development 429 266 0.62 3971.7 21405.3 153.9 6017.6 Animal welfare 429 117 0.27 1503.7 11025.4 0 964.5 EA meta and organization 429 74 0.17 771.4 7106.9 0 261.7 Long term &amp; AI 429 78 0.18 975.3 9963.9 0 345.6 Other 429 47 0.11 874.4 12863.9 0 25.3 Below, we depict the amounts and density of donations for each category. Details The vertical axis is on a logarithmic scale. The width of the violin plot depicts the smoothed density. In the box, the horizontal lines represent medians for each, lower and upper margins of the box 25th and 75th percentiles, “whisker” lines extends from the box to to the largest (smallest) value no further than 1.5 \\(\\times\\) the inter-quartile range, and large dots represent outlying points beyond the edge of the whiskers. ##TODO -- #sort by reverse frequency of donations to a cause #TODO: bottom code and change the scale on this, time permitted ( don_by_cause_viol &lt;- eas_new %&gt;% filter(num_named_dons&gt;0) %&gt;% select(where_don_vars, action_gwwc_f) %&gt;% gather(cause, don, -action_gwwc_f) %&gt;% ggplot() + aes(cause, don) + geom_violin() + geom_boxplot() + ylab(&quot;Donation amount&quot;) + geom_point(size = 0.30, colour = &quot;#0c4c8a&quot;, position = position_jitter(seed = 42, width = 0.3, height = 0.01)) + scatter_theme + scale_y_log10(labels = scales::label_number_si(prefix = &quot;$&quot;), n.breaks = 10) + scale_x_discrete(labels = function(x) str_wrap(all_char_labels, width = 10)) + labs(title = &quot;Donation amounts by category: see description above&quot;) ) #Todo (Low to medium) ... @oska: if it&#39;s easy-ish, maybe gganimate this one across years? #@David: Kinda difficult to do this as the variables in where_don_vars don&#39;t seem to align with eas_all #@oska -- it is there, in variables like `donate_[charity]_year` but it would require considerable data cleaning work. Will ask/see if it&#39;s worth it. We also check whether donations to each cause (incidence and amounts) vary by whether the person (ever) took a GWWC pledge. #TODO -- High Priority (@oska): clean up the below to be more readable, add the mean and a CI for the mean ( don_by_cause_viol_gwwc &lt;- eas_new %&gt;% filter(num_named_dons&gt;0 &amp; !is.na(action_gwwc_f)) %&gt;% select(where_don_vars, action_gwwc_f) %&gt;% gather(cause, don, -action_gwwc_f) %&gt;% ggplot() + aes(cause, don, color=action_gwwc_f) + scale_color_discrete(name=&quot;GWWC pledge&quot;, labels=c(&quot;No&quot;, &quot;Yes&quot;)) + geom_violin() + geom_boxplot(notch=TRUE) + geom_point(size = 0.30, colour = &quot;#0c4c8a&quot;, position = position_jitter(seed = 42, width = 0.3, height = 0.01)) + scatter_theme + scale_y_log10() + scale_x_discrete(labels = function(x) str_wrap(all_char_labels, width = 10)) ) Difference from previous plots Comparing this to the “Donation amounts by category…” plot, Here, the lower and upper margins of the (now ‘notched’) box present an estimate of 95% confidence interval for medians (for those reporting at least one category of donations and reporting GWWC status). Below, we tabulate donations for each cause, by group.15 don_stats_by_gwwc Table 3.4: Donations by category (where indicated), by GWWC Variable N Responses Share positive Mean Median N Responses Share positive Mean Median Test GWWC Pledge No Yes Global health + development 241 0.59 2832.7 1144.1 187 0.66 5450.2 1854.9 F=1.573 Animal welfare 241 0.22 1480.4 867.1 187 0.34 1531 495.5 F=0.002 EA meta and organization 241 0.12 696.4 553.1 187 0.24 872.1 335.7 F=0.064 Long term &amp; AI 241 0.13 1144.7 830 187 0.25 762.1 274.8 F=0.155 Other 241 0.06 1289.5 1103.3 187 0.17 344.1 96.8 F=0.567 Next, as above, but for donation incidence:16 ddon_stats_by_gwwc Table 3.5: Binary: Indicated donating to category, by GWWC Variable N Responses Donated to… ? N Responses Donated to… ? Test GWWC Pledge No Yes Global health + development 241 187 X2=2.399 … No 100 41% 63 34% … Yes 141 59% 124 66% Animal welfare 241 187 X2=7.897*** … No 189 78% 123 66% … Yes 52 22% 64 34% EA meta and organization 241 187 X2=8.283*** … No 211 88% 143 76% … Yes 30 12% 44 24% Long term &amp; AI 241 187 X2=9.831*** … No 210 87% 140 75% … Yes 31 13% 47 25% Other 241 187 X2=11.681*** … No 226 94% 155 83% … Yes 15 6% 32 17% # #TODO -- High Priority (@oska): -- the below is a mess... we want both the frequency table and test for each of these ... but how to do it. I feel like I&#39;ve done this before. maybe the function in rstuff `fisherme` would help? # TODO (high-medium): Once we get it to work, do similar plots and tests for different &#39;which cause&#39; comparisons ... fisher_cats &lt;- eas_new %&gt;% filter(num_named_dons&gt;0) %&gt;% dplyr::select(all_of(where_don_vars)) %&gt;% lapply(janitor::fisher.test, y = eas_new$action_gwwc_f[eas_new$num_named_dons&gt;0], simulate.p.value=TRUE) Discussion of results above, tests … As suggested in the first of the two tables above, among those who report a charity category, those who took the GWWC pledge tend to give … 3.7 Donations: plans/aspirations vs. actual (reported) amounts To consider: Should this section be repeated every year, done in an aggregated fashion across years, or skipped going forward (just referring back to 2019-20) #filtering and shaping functions f_don_plan_by_year &lt;- function(df=eas_all, years=latest_years) { #adjusting for comparing planned and actual donation for same year in question (but not always for &#39;same individuals&#39;) {df} %&gt;% select(year, donation_c, donation_plan_c) %&gt;% gather(donation_type, value, -year) %&gt;% mutate(year = if_else(donation_type == &quot;donation_plan_c&quot;, year, year-1)) %&gt;% mutate(year = fct_rev(as.factor(year)), donation_type = fct_recode(donation_type, &quot;Planned Donation&quot; = &quot;donation_plan_c&quot;, &quot;Donation&quot; = &quot;donation_c&quot;)) %&gt;% filter(year %in% years) } f_don_last_3 &lt;- function(df=eas_all, years=last_3_years) { #this is for comparing to &#39;planned donation&#39; (next year) df %&gt;% dplyr::filter(year %in% years) %&gt;% group_by(year) %&gt;% select(year, donation_c, donation_plan_c) %&gt;% gather(donation_type, value, -year) } f_next_d_don &lt;- function(df=eas_all) { #same as f_don_last_3, but instead of gather it constructs a differenced variable `next_d_don` df %&gt;% dplyr::filter(year %in% last_3_years) %&gt;% select(year, donation_c, donation_plan_c) %&gt;% transmute(next_d_don = donation_plan_c - donation_c) } #Construct key tibbles to use in comparing planned and actual for &#39;this year&#39; demographics &lt;- c(&#39;age&#39;, &#39;gender&#39;, &#39;country&#39;, &#39;employ_status&#39;) # Filtering for those present in both datasets planned_actual_ly_ty &lt;- eas_all %&gt;% filter(year %in% c(year_n, year_n-1) &amp; !is.na(ea_id)) %&gt;% select(ea_id, donation_c, donation_plan_c, year) %&gt;% distinct() %&gt;% group_by(ea_id) %&gt;% filter(n() == 2) %&gt;% # Filter for those appearing in both years pivot_wider(names_from = &quot;year&quot;, values_from = c(&quot;donation_c&quot;, &quot;donation_plan_c&quot;)) %&gt;% # Remove unnecessary columns select(-donation_plan_c_2020, -donation_c_2019) %&gt;% # drop_na() %&gt;% # Ensure that each participant had planned donation from 2019 and actual donation from 2020 # TODO - fix, this is dropping everything rename(donation_last_survey_year = donation_c_2020, planned_donation_prior_survey_year = donation_plan_c_2019) %&gt;% #2022: renamed from &#39;donation_ly&#39; and &#39;planned_donation_ly&#39; for greater clarity # Add demographic information left_join(., select(eas_new, all_of(demographics), ea_id, action_gwwc, start_date, end_date, income_c), by = &quot;ea_id&quot;) #Convert to long format for ggplot planned_actual_ly_ty_l &lt;- planned_actual_ly_ty %&gt;% group_by(ea_id) %&gt;% gather(donation_type, value, donation_last_survey_year, planned_donation_prior_survey_year) ## helper functions f_ly_hyp &lt;- function(df) { #2019 data for donation difference df %&gt;% filter(donation_last_survey_year &gt; 0 &amp; planned_donation_prior_survey_year&gt;0) %&gt;% #positive don in each year transmute(don_diff = donation_last_survey_year - planned_donation_prior_survey_year) #only the difference is used; this adds an &#39;attribute&#39; to this object } #test_rep_don_diff_mn_19 #point hypothesis of 0 mean (+attribute) #1000 replications of the relevant &#39;data&#39; #test_rep_don_diff_med_19: as above but for median #test_rep_next_d_don_mn_18_20 #for actual vs *next* year&#39;s plan (means) #test_rep_next_d_don_med_18_20 ... (medians) #### Linked tests: New Purr testing framework ##### # ...Alternate between testing mean and median = 0 #### mean_zero_hyp &lt;- list(null = &quot;point&quot;, mu = 0) med_zero_hyp &lt;- list(null = &quot;point&quot;, med = 0) hyps &lt;- list(mean_zero_hyp, med_zero_hyp) # ...Stats to calculate ##### stat_mean &lt;- list(list(stat = &quot;mean&quot;)) stat_median &lt;- list(list(stat = &quot;median&quot;)) bs_1000 &lt;- list(reps = 1000, type = &quot;bootstrap&quot;) #dataframes for testing &#39;current less next donation&#39; and &#39;actual less planned donation&#39; df_next_don &lt;- eas_all %&gt;% f_next_d_don df_don_diff &lt;- planned_actual_ly_ty %&gt;% f_ly_hyp n &lt;- 4 # Total number of tests ... mean and median for each dataframe (better to softcode this?) responses &lt;- c( rep(&quot;don_diff&quot;, n/2), rep(&quot;next_d_don&quot;, n/2)) p_value_directions &lt;- rep(&quot;two_sided&quot;, n) # ... Functionalize #### dfs &lt;- list( rep(list(df_don_diff), n/2), rep(list(df_next_don), n/2)) linked_df_labels &lt;- c(rep(&quot;Last year-this year linked responses&quot;, 2), rep(&quot;Last 3 years all responses&quot;, 2)) linked_test_var_type &lt;- c(rep(&quot;Actual vs Planned&quot;, 2), rep(&quot;&#39;Next year&#39; vs Current&quot;, 2)) linked_tests_df &lt;- tibble(df = do.call(c, dfs), # Dataframes (needs tidying) # Stats to calculate stat = rep(c(stat_mean, stat_median), n/2), # Hypotheses to test hypothesis = rep(hyps, n/2), # Samples to generate gen = rep(list(bs_1000), n), # Outcome variables response = responses, # Direction for p-value calculation p_val_dir = rep(&quot;two_sided&quot;, n)) # .... actually run tests and collect pvalues etc #### linked_tests_df &lt;- linked_tests_df %&gt;% mutate(results = pmap(., htest_infer_sim)) #htest_infer_sim was defined in `hypothesis_test.R`; it runs the steps in the Infer testing package with options selected based on the content of the arguments. linked_tests_results &lt;- extract_hyp_results(linked_tests_df) %&gt;% #extract and label key results for reporting and plotting mutate(data_label = linked_df_labels, data_type = linked_test_var_type) # ... make a tibble of the relevant dataframes and &#39;test formula elements&#39; #### unlinked_tests_df &lt;- tibble(df = do.call(c, unlinked_data), formula = rep(list(unlinked_formula), n), hypothesis = rep(hyp_unlinked, n), gen = rep(list(perm_200), n), stat = c(rep(d_order_diff_means, n-1), d_order_next_diff_means), p_val_dir = rep(&quot;two_sided&quot;, n)) # Column labels rename_test_results &lt;- c(&quot;Statistic&quot; = &quot;stat&quot;, &quot;Null type&quot; = &quot;null&quot;, &quot;Null value&quot; = &quot;null_value&quot;, &quot;Point estimate&quot; = &quot;point_estimate&quot;, &quot;CI Lower&quot; = &quot;lower_ci&quot;, &quot;CI Upper&quot; = &quot;upper_ci&quot;, &quot;P-value&quot; = &quot;p_value&quot;, &quot;Sample&quot; = &quot;data_label&quot;) # This can be used for plotting full_test_results &lt;- dplyr::bind_rows(linked_tests_results, unlinked_diff_in_means_results, unlinked_diff_in_medians_results) %&gt;% select(-c(order)) %&gt;% mutate(across(c(stat, null, p_val_dir), ~ snakecase::to_sentence_case(.x))) # This forms the basis for tables/displaying stats full_test_results_clean &lt;- full_test_results %&gt;% select(-c(reps, type, formula, p_val_dir, response)) %&gt;% rename(!!rename_test_results) %&gt;% mutate(Statistic = str_replace_all(Statistic, c(&quot;means&quot; = &quot;Mean&quot;, &quot;medians&quot; = &quot;Median&quot;, &quot;Diff&quot; = &quot;Difference&quot;))) #making tables #For linked tests: current_next_test_results_clean &lt;- full_test_results_clean %&gt;% filter(data_type == &quot;&#39;Next year&#39; vs Current&quot;) %&gt;% select(-c(data_type, null_dist)) planned_v_actual_test_results_clean &lt;- full_test_results_clean %&gt;% filter(data_type == &quot;Actual vs Planned&quot;) %&gt;% select(-c(data_type, null_dist)) planned_v_actual_test_table &lt;- planned_v_actual_test_results_clean %&gt;% select(-c(`Null value`, `Null type`, `Sample`)) %&gt;% kable(caption = &quot;Actual minus planned donations for last year, linked participants (this year and last year)&quot;, digits=c(0,0,0,3)) %&gt;% kable_styling() current_next_test_table &lt;- current_next_test_results_clean %&gt;% select(-c(`Null value`, `Null type`, `Sample`)) %&gt;% kable(digits=c(0,0,0,0,3), caption = &quot;Planned minus last year&#39;s donation, 2018-20, all participants who report donations&quot;) %&gt;% kable_styling() #making tables for UNLINKED tests: planned_actual_unlinked_results_table &lt;- full_test_results_clean %&gt;% arrange(match(Sample, c(&quot;Full sample (Prior donation years)&quot;, &quot;Involved before last year (Prior-year don)&quot;, &quot;GwwC only (Prior-year don)&quot;, &quot;&#39;Matched individuals&#39;&quot;))) %&gt;% filter(`Null type` == &quot;Independence&quot; &amp; data_type == &quot;Actual - Planned&quot; ) %&gt;% select(-c(data_type, null_dist, `Null value`, `Null type`)) %&gt;% select(Sample, Statistic, everything()) %&gt;% kable(caption = &quot;Actual versus Planned donation distributions: permutation tests&quot;, digits=c(0,0,0,0,0,3)) %&gt;% kable_styling() next_current_unlinked_results_table &lt;- full_test_results_clean %&gt;% filter(`Null type` == &quot;Independence&quot; &amp; data_type == &quot;&#39;Next year&#39; - &#39;this year&#39;&quot; ) %&gt;% select(-c(data_type, null_dist, `Null value`, `Null type`)) %&gt;% select(Sample, Statistic, everything()) %&gt;% kable(caption = &quot;&#39;Next year (plan)&#39; - &#39;this year&#39; donation distributions: permutation tests&quot;, digits=c(1,1,1,3)) %&gt;% kable_styling() Do people meet or exceed the amount they intended or planned to donate for the next year? In recent surveys, we have asked “In [current year] how much do you currently plan to donate?”. We also ask “in [previous year], roughly how much money did you donate?”. Note: timings of surveys The EA surveys have been released at various points in the year: In 2017, the survey was released in April; thus the ‘plan’ was reported only about 1/3 of the way through the year (or slightly later, depending on response time). In 2018, the survey was released in May. In 2019, it was released in August, about 3/4 of the way throughout the year. Thus, for each of these years, the year-to-year comparison may tell us something about whether people lived up to their plans. 2019 Planned vs. actual: Individuals present in both surveys We first consider those 441 respondents who can be matched across the 2019 and 2020 surveys.17 The plots below cover only respondents who appear in both samples and provide planned and actual donation values. These individuals make up 22.9% of the total respondents that appear in the 2020 sample and 15% of the total respondents across 2019 and 2020.18 # Create plots for planned and actual donations matched across 2019 scales_point_density_min50 &lt;- list(limits = c(50, 500000), trans = scales::pseudo_log_trans(base=10), breaks = breaks, labels = scales::dollar_format()) planned_actual_ly_ty_density &lt;- planned_actual_ly_ty_l %&gt;% rowwise() %&gt;% mutate(value = max(value, 50)) %&gt;% ungroup() %&gt;% ggplot() + geom_density(aes(x = value, fill = donation_type), alpha = 0.5) + do.call(scale_x_continuous, scales_point_density_min50) + scale_y_continuous(breaks = density_breaks, expand = c(0,0)) + ggtitle(&quot;Actual vs Planned last year&#39;s donations&quot;, subtitle = &quot;Donations bottom-coded at $50; subset: those who can be matched across surveys)&quot;) + theme(legend.position = &quot;bottom&quot;, legend.margin=margin(t = -0.6, unit=&#39;cm&#39;)) + # Shift legend position up xlab(&quot;&quot;) + ylab(&quot;Density&quot;) + scale_fill_discrete(name = &quot;&quot;, labels = to_title_case(unique(planned_actual_ly_ty_l$donation_type))) # Define same parameters for x and y axis scales_point_density &lt;- list(limits = c(0, max_lim), trans = scales::pseudo_log_trans(base=10), breaks = breaks, labels = scales::dollar_format()) planned_actual_ly_ty_pointdensity &lt;- planned_actual_ly_ty %&gt;% rowwise() %&gt;% mutate(planned_donation_prior_survey_year = max(planned_donation_prior_survey_year, 50), donation_last_survey_year = max(donation_last_survey_year, 50)) %&gt;% ungroup() %&gt;% ggplot(aes(y = donation_last_survey_year, x = planned_donation_prior_survey_year)) + ggpointdensity::geom_pointdensity(adjust = 0.25) + geom_abline(slope = 1, intercept=0, linetype = &quot;dotted&quot;) + geom_smooth() + do.call(scale_x_continuous, scales_point_density_min50) + do.call(scale_y_continuous, scales_point_density_min50) + scale_color_viridis_c(&quot;Neighbours&quot;) + #scale_size_continuous(&quot;Income&quot;, labels = scales::label_number_si()) + ylab(&quot;Actual last years&#39; Donation (bottom-coded @ $50)&quot;) + xlab(&quot;Planned last year&#39;s donation from prior year (bottom-coded @ $50)&quot;) + ggtitle(&quot;Planned &amp; actual donations (cross-survey matches)&quot;) #@oska I added a geom_smooth. If you can get it to work with the income-size and legends looking good, let&#39;s put that back (TODO) #We can also trim the right horizontal axis perhaps (maybe that can be set more generally above?) Below, we plot planned and actual 2019 donations for these respondents.19 planned_actual_ly_ty_density We separate the above graph by whether the individual made a GWWC pledge: ( planned_actual_gwwc &lt;- planned_actual_ly_ty_l %&gt;% rowwise() %&gt;% mutate(value = max(value, 50)) %&gt;% ungroup() %&gt;% filter(!is.na(action_gwwc)) %&gt;% mutate(action_gwwc = as.factor( if_else(action_gwwc == 1, &quot;GWWC Pledge&quot;, &quot;No GWWC Pledge&quot;) ) )%&gt;% # mutate(value = value + 1) %&gt;% ggplot() + geom_density(aes(x = value, fill = donation_type), alpha = 0.5) + scale_x_log10(labels = scales::label_number_si(prefix = &quot;$&quot;)) + ggtitle(&quot;Actual vs Planned 2019 donations by &#39;made GWWC pledge&#39;&quot;, subtitle=&quot;Linked individuals, log scale&quot;) + xlab(&quot;Donation value, bottom-coded at $50&quot;) + ylab(&quot;Density&quot;) + facet_grid(action_gwwc ~ . ) + scale_fill_discrete(name = &quot;&quot;, labels = to_title_case(unique(planned_actual_ly_ty_l$donation_type))) ) #TODO (\\@oska) -- maybe do this specifically for a year in which there is a large gap in timing -- perhaps 2018 is the best as it was &lt;ay (1/2 the year) and we think it&#39;s a reliable data year #TODO: Med-high -- test for difference in planned and actual (a &#39;shift&#39;) and ideally test for a difference in difference between GWWC and non-GWWC We ran a series of simulation-based ‘permutation tests’ to consider compare the medians and means of the distributions of planned and actual donations for these linked individuals. Results are included in the table “Actual minus planned donations for 2019, linked participants (2019 - 2020)” in the section Planned vs. actual: All respondents. … 20 Donations versus plans (same individuals, linked) While the graphs and figures above help us understand whether the distribution of planned and actual gifts differ, it does not tell us whether any individual’s donation meets or exceeds his or her plan. As we are considering individuals present in both surveys, we can connect their donation responses across years. The graph below shows the distribution over the difference in planned and actual 2019 donations for those matched across the years. Here a negative value corresponds to an actual donation being lower than planned. #TODO [Medium-High] -- incorporate it in so it will work for split plots, for &#39;same year&#39;, etc. m_dd &lt;- planned_actual_ly_ty %&gt;% transmute(don_diff = donation_last_survey_year - planned_donation_prior_survey_year) %&gt;% ungroup() %&gt;% dplyr::summarize(mn_dd=mean(don_diff, na.rm=TRUE), med_dd = median(don_diff, na.rm=TRUE) ) ( actual_planned_2019 &lt;- planned_actual_ly_ty %&gt;% transmute(don_diff = donation_last_survey_year - planned_donation_prior_survey_year) %&gt;% ggplot(aes(x = don_diff)) + geom_density(alpha=0.5, fill=&quot;blue&quot;) + scale_x_continuous(trans = pseudo_log_trans(base=10), breaks = c((-1)*breaks*2, breaks*2), labels = label_number_si(prefix = &quot;$&quot;)) + geom_vline(xintercept=m_dd$mn_dd, size=1.5, color=&quot;green&quot;) + # this code is lame, we can improve it geom_vline(xintercept=m_dd$med_dd, size=1.5, color=&quot;red&quot;) + # this code is lame, we can improve it geom_vline(xintercept=0) + # coord_flip() + labs(title = &quot;Last year&#39;s donations: actual minus previously planned&quot;, caption = &quot;Red line: Median, Green line: Mean&quot; ) + xlab(&quot;Actual - planned for same year&quot;) + ylab(&quot;&quot;) ) # TODO: keep improving this guy Planned and actual donations are… highly correlated (\\(\\rho =\\) 0.948). … In fact, the mean difference between donation and plan is 1,139 USD in excess of plan (the green line), while the median of the differences is 67.9 USD. Considering that the zeroes might have been quick and uncareful mis-responses, we repeat the same plot for those who report positive planned and actual donations in the consecutive years, and compare these for GWWC pledgers versus non-pledgers: ( actual_planned_2019_no_0_bygwwc &lt;- planned_actual_ly_ty %&gt;% filter(!is.na(action_gwwc)) %&gt;% filter(donation_last_survey_year&gt;0 &amp; planned_donation_prior_survey_year&gt;0) %&gt;% mutate(don_diff = donation_last_survey_year - planned_donation_prior_survey_year) %&gt;% ggplot(aes(x = don_diff, y = as.factor(action_gwwc), fill = factor(stat(quantile)))) + stat_density_ridges( geom = &quot;density_ridges_gradient&quot;, calc_ecdf = TRUE, quantiles = 4, quantile_lines = TRUE ) + scale_fill_viridis_d(name = &quot;Quartiles&quot;) + scale_x_continuous(trans = pseudo_log_trans(base=10), breaks = c((-1)*breaks*2, breaks*2), labels = label_number_si(prefix = &quot;$&quot;)) + geom_vline(xintercept=0) + ggtitle(&quot;2019 donations (no zeroes): actual minus planned, by GWWC-pledge&quot;) + xlab(&quot;&quot;) + ylab(&quot;&quot;) ) … We next present a scatterplot of planned versus actual donations for 2019, for those who can be matched across surveys. In the figure below, the brightness of a color indicates the density of respondents (number of ‘neighbors’) with a particular combination of planned and actual donations. planned_actual_ly_ty_pointdensity Overall, the plot is more or less centered around the 45 degree line of ‘plans=actual’. There are noticeable departures in each direction, but these seem to balance out. Thus, we might loosely conclude that ‘on average 441 individuals who can be matched across years tend to donate an amount close to what they planned’. However, there may nonetheless be important differences, so we test further. Below, we plot donations for these linked individuals – actual donations are on the left, and planned donations are on the right. We overlay a ‘violin’ density plot (widths depicts the frequencies). Medians are depicted in red dots, and the boxes depict 25th and 75th percentiles. The lines show each individual’s donation (on the left) connected to her plan (on the right). The plot also reports on a Wilcoxon signed-rank test (for paired data). ( matched_dons_wilcoxon &lt;- planned_actual_ly_ty_l %&gt;% mutate(donation_type = to_title_case(donation_type)) %&gt;% ggstatsplot::ggwithinstats( x = donation_type, y = value, type = &quot;nonparametric&quot;, paired = TRUE, point.path.args = list(alpha = 0.1, linetype = &quot;solid&quot;), ) + do.call(scale_y_continuous, scales) + xlab(&quot;&quot;) + ylab(&quot;&quot;) + scale_fill_discrete(name = &quot;&quot;) ) w_signed_test_planned_actual &lt;- wilcox.test(x = planned_actual_ly_ty$donation_last_survey_year, y = planned_actual_ly_ty$planned_donation_prior_survey_year, alternative = c(&quot;greater&quot;), mu = 0, paired = TRUE, exact = NULL, correct = TRUE, conf.int = TRUE, conf.level = 0.95, tol.root = 1e-4, digits.rank = Inf) #a Wilcoxon signed rank test of the null that the distribution of ... x - y (in the paired two sample case) is symmetric about mu is performed. #Here the CI estimates &#39;the median of the difference between a sample from x and a sample from y.&#39; The nonparametric tests reported above find a statistically significant difference: actual donations tend to exceed planned donations in this sample, and this difference is unlikely to be due to chance. The ‘pseudo-median’ of this difference is estimated as 281 USD with 95% lower CI bound 162. The “matched-pairs rank-biserial correlation” is also bounded between about 0.17 and 0.41, suggesting that “actual donation exceeds planned donation” is more likely than “planned exceeds actual” (in the population that this is drawn from). # For those who report a donation in each year... planned_actual_ly_ty_no_0 &lt;- planned_actual_ly_ty %&gt;% filter(donation_last_survey_year&gt;0 &amp; planned_donation_prior_survey_year&gt;0) w_signed_test_planned_actual_no0s &lt;- wilcox.test(x = planned_actual_ly_ty_no_0$donation_last_survey_year, y = planned_actual_ly_ty_no_0$planned_donation_prior_survey_year, alternative = c(&quot;greater&quot;), mu = 0, paired = TRUE, exact = NULL, correct = TRUE, conf.int = TRUE, conf.level = 0.95, tol.root = 1e-4, digits.rank = Inf) #a Wilcoxon signed rank test of the null that the distribution of ... x - y (in the paired two sample case) is symmetric about mu is performed. #Here the CI estimates &#39;the median of the difference between a sample from x and a sample from y.&#39; We next present simulation-based tests for whether the mean and median of the individual ‘actual minus planned’ donations exceeds or falls below zero. planned_v_actual_test_table Table 3.6: Actual minus planned donations for last year, linked participants (this year and last year) Statistic Point estimate CI Lower CI Upper P-value Mean 1587 -279 3453.648 0 Median 141 23 259.456 0 linked_ddmn &lt;- linked_tests_results %&gt;% filter(response==&quot;don_diff&quot; &amp; stat==&quot;mean&quot;) linked_ddmed &lt;- linked_tests_results %&gt;% filter(response==&quot;don_diff&quot; &amp; stat==&quot;median&quot;) The mean of ‘actual minus planned’ donations is 1,587 USD, with simulation-based (bootstrapped) confidence intervals [-279, 3,454], with corresponding p-value 0.138. For the median of this difference we have point estimate 141 USD, with simulation-based (bootstrapped) confidence intervals [22.5, 259], with corresponding p-value 0.102. Thus, the evidence points towards ‘actual donations exceeding planned donations for those EAs who can be linked across the past two years’. Planned vs. actual: All respondents (across relevant years) Those who responded to both 2019 and 2020 surveys (and left emails both times) might tend to be the more engaged EAs, suggesting that the above figures may be biased towards more ‘fulfilled plans’.21 Thus, we next overlay the planned and actual donations for all respondents across both surveys.22 Show me more Indeed, ‘2019 respondents who entered a planned donation amount’ (call these ‘2019-planners’) may not be precisely representative of the population of interest. Still, we might at least seek an ‘internally-valid’ measure of the ‘distribution of actual 2019 donations for the 2019-planners’, and compare their actual to their planned 2019 donations. This will still be imperfect: the composition of the 2019 and 2020 respondents may differ, as discussed elsewhere. Thus, the distribution of ‘reported 2019 donations for those who completed the survey in 2020’ (call these ‘2020-reporters’) may be different from the distribution of the actual 2019 donations made by 2019-planners. The direction and nature of this bias is unclear; we might get some hints at this by comparing the reported donations in 2018 and 2019 respectively by 2019-reporters and 2020-reporters. If the distribution of donations changes little from year to year, we might worry less about this bias. We defer this for future work. Below, we also report these measures for those who joined EA only before 2019 (plausibly a more stable group). ( dons_planned_across_all &lt;- eas_all %&gt;% f_don_plan_by_year %&gt;% ggplot(aes(x = value, y = as.factor(year), fill = donation_type)) + geom_density_ridges(alpha=.6, color = &quot;black&quot;, quantile_fun = median, quantile_lines = TRUE, rel_min_height = 0.005) + # geom_vline_med(x) + scales_set + ridge_bottom_opts + xlab(&quot;&quot;) + guides(fill = guide_legend(override.aes = list(linetype = 0))) + labs(title= &quot;Density of planned and actual donations for each year&quot;, subtitle = &quot;Vertical lines: medians for the year and donation type&quot;, caption = &quot;Donations bottom-coded at $50&quot;) ) Discussion of above chart … For 2019 (2020 survey ‘actual’ and 2019 survey ‘planned’) and 2018 (2019 survey ‘actual’ and 2018 survey ‘planned’), the histograms of planned and actual donations line up approximately (although planned donations tend to be a bit higher). However, for 2017 (2018 survey ‘actual’ and 2017 survey ‘planned’), the planned donation distribution appears far lower. This seems likely to result from a different response and a different composition between the 2017 and 2018 responses.^[There was an increase in the sample size and the response rate to the donation questions from 2017 to 2018 below. We report this in the bookdown ‘robustness’ appendix.] (Thus we will not include 2017 in our “planned versus actual” comparisons.) As noted, for the remaining relevant donation years (2018 and 2019) the median donation is somewhat lower than the median planned donation, suggesting under-performance relative to plans. Over the most recent years, how do the distributions of planned versus actual donations differ? In spite of the caveats above, we consider and test whether the distribution of planned donations for a year exceeds or falls short of actual donations, pooling across recient years. We do this separately both overall, and excluding those who joined EA only before 2019 (plausibly a more stable group). #TODO -- medium-high priority: some depiction of quantiles/cutoffs within each smoothed histogram (for all the ones below, even the faceted ones). See, e.g., https://stackoverflow.com/questions/57563692/combining-facet-wrap-and-95-area-of-density-plots-using-ggplot2/57566951#57566951 dons_plan_hist_opts &lt;- function(df) { df %&gt;% ggplot(aes(x = value, fill = donation_type)) + geom_density(alpha=.35) + scales_set + #geom_vline_med(x) + ridge_bottom_opts + xlab(&quot;&quot;) } #crappy workaround here: x &lt;- eas_all %&gt;% filter(year %in% last_3_years) %&gt;% f_don_plan_by_year xo &lt;- eas_all %&gt;% filter(year_involved&lt;2019) %&gt;% filter(year %in% last_3_years) %&gt;% f_don_plan_by_year x_med_don &lt;- median(x$value[x$donation_type== &quot;Donation&quot;], na.rm=TRUE) x_med_don_plan &lt;- median(x$value[x$donation_type== &quot;Planned Donation&quot;], na.rm=TRUE) xo_med_don &lt;- median(x$value[xo$donation_type== &quot;Donation&quot;], na.rm=TRUE) xo_med_don_plan &lt;- median(xo$value[x$donation_type== &quot;Planned Donation&quot;], na.rm=TRUE) dons_planned_last_2_years &lt;- eas_all %&gt;% filter(year %in% last_3_years) %&gt;% f_don_plan_by_year %&gt;% rowwise() %&gt;% mutate(value = max(value, 50)) %&gt;% ungroup() %&gt;% dons_plan_hist_opts + geom_vline(xintercept=x_med_don, size=.5, color=&quot;green&quot;) + geom_vline(xintercept=x_med_don_plan, size=.75, color=&quot;pink&quot;) + ggtitle(&quot;2018-19&quot;) dons_planned_last_2_years_no_new &lt;- eas_all %&gt;% filter(year_involved&lt; year_n-1) %&gt;% filter(year %in% last_3_years) %&gt;% f_don_plan_by_year %&gt;% dons_plan_hist_opts + geom_vline(xintercept=xo_med_don, size=.5, color=&quot;green&quot;) + geom_vline(xintercept=xo_med_don_plan, size=.75, color=&quot;pink&quot;) + ggtitle(&quot;2018-19&quot;) ( dons_planned_last_2_years_arr_new &lt;- ggarrange(dons_planned_last_2_years, dons_planned_last_2_years_no_new, ncol = 2, nrow=1, labels=c(&quot;All&quot;, &quot;Involved pre-last-year&quot;), label.x=0.0, label.y=0.85, legend=&quot;bottom&quot;, common.legend=TRUE ) + labs(title= &quot;Density of planned and actual donations, split by year-invo&quot;, subtitle = &quot;Vertical lines: medians for the group and donation type&quot;, caption = &quot;Donations bottom-coded at $50&quot;) ) #todo (@oska) -- finish this; separate aligned plots for # - is it doing what I think? I think we want to use 2020 for the sums in &#39;actual&#39; but not in &#39;planned&#39; # - align the above, fix labels # - report some more stats within each as &#39;bars&#39; or shading (geom lines could also be good) # - stat tests for each Above, we give the density of planned and actual donations, split by year-involved. Vertical lines represent medians for the group and donation type (green=donation, pink=planned donations). Donations are bottom-coded at 50 USD. Again, for both groups (where the survey entries are linked) the planned donation distribution appears to be somewhat higher than the actual distribution, although the difference is not dramatic. We present the results of simulation-based permutation tests below. (Explanation in fold). Explanation of permutation tests We use permutation tests for testing whether the median (and mean) of planned donations exceeded/fell short of the mean for actual donations, but using the data from different years’ surveys (without connected individuals). The null hypothesis (for 2019 donations) is that: ‘there is no difference in the median donation, in our survey sample, between actual 2019 donations (reported in 2020) and planned 2019 donations (reported in 2019).’ Suppose we maintain the hypothesis that ‘individuals appeared in the 2019 survey versus the 2020 survey ’as if randomly drawn’’, and we consider that under the null hypothesis “the distribution of planned and actual donations is identical” (this may be a stronger assumption than needed). The permutation procedure repeatedly simulates a distribution of planned and actual donations that is consistent with this null, by using our original donation amount data and randomly re-assigning each observation to either ‘planned’ or ‘actual’. For each ‘simulated null distribution’ we can then compute the targeted statistic (difference between mean/median donation between the two groups). We can plot this ‘simulated null distribution of differences’ and consider ‘how often do we observe a difference as extreme as the ’point estimate’ from our actual data’? This yields the p-values reported below. The confidence intervals for the differences come from simply using the 95% interval range from the simulated distribution, shifted to be centered around our point estimate. The figure below presents the simulated null distribution of differences in medians arising from this procedure, for the full sample (2018 - 2019 donation years). NOTE: we lost the image here Explanation The red line gives the point estimate of ‘differences in median between planned and actual’ from our data. The grey bins present the ‘(simulated) distribution of differences between planned and actual, under the null hypothesis that planned and actual are drawn from the same distribution’. The ‘pink areas’ depict the area of the simulated null distribution that is ‘more extreme’ than our point estimate; this iis the area represented by the ‘p-value’ of a two-tailed hypothesis test. Finally the turquoise should represent a 95% CI for the actual median; essentially this comes from a simulated distribution under the hypothesis that the true difference between the medians is exactly equal to our point estimate. Note that this turquoise region just crosses the 0, again confirming that we ‘just barely cannot reject the null’, but that a Bayesian analysis would probably put a lot of probability mass on fairly substantial differences. In the table below we summarize the results of this test for means and medians, and for four distinct subsamples. planned_actual_unlinked_results_table Table 3.7: Actual versus Planned donation distributions: permutation tests Sample Statistic Point estimate CI Lower CI Upper P-value ‘Matched individuals’ Difference in Mean 1449 -2599 5496 0.504 ‘Matched individuals’ Difference in Median 94 -757 945 0.868 Full sample (2018-19 donation years) Difference in Mean -2354 -7288 2579 0.366 GwwC only (2018-19 don) Difference in Mean -235 -10853 10384 0.970 Involved before last year (2018-19 don) Difference in Mean -1293 -6742 4156 0.684 Full sample (2018-19 donation years) Difference in Median -458 -608 -308 0.000 GwwC only (2018-19 don) Difference in Median -1027 -1469 -586 0.000 Involved before last year (2018-19 don) Difference in Median -157 -336 23 0.008 # report on #test_rep_mean_don_19_20 #test_rep_med_don_19_20 #test_rep_mean_don_19_20_gwwc #test_rep_med_don_19_20_gwwc #test_rep_mean_don_19_20_nonew #test_rep_med_don_19_20_nonew For the full sample, for the subset who have been in EA since before 2019, and for GWWC pledgers, both the mean and median donations […] fall short of the planned donations. Overall, the difference in medians is bounded between about … … re-state results… This contrasts with 23 3.8 Donations versus next year’s plans […] In each of the years where it was asked, most respondents who answered the retrospective donation question also answered the ‘planned for this year’ question. We can see how these tend to relate; we may particularly consider whether 2020 donations are expected to be higher or lower than 2019, in light of recent events. Below we overlay the distribution of ‘last year’s donations’ and ‘planned current year’ donations for the most recent surveys. next_don_lab &lt;- c(donation_c = &quot;Don: Last year&quot;, donation_plan_c = &quot;Don: This year (plan)&quot;) ( dons_v_next_last_2_years &lt;- eas_all %&gt;% f_don_last_3 %&gt;% ggplot(aes(x = value, y = as.factor(year), fill = donation_type)) + geom_density_ridges(alpha=.6, color = &quot;black&quot;, quantile_fun = median, quantile_lines = TRUE) + # geom_vline_med(x) + scales_set + ridge_bottom_opts + guides(fill = guide_legend(override.aes = list(linetype = 0))) + labs(title= &quot;Density of last year vs current year planned donations&quot;, subtitle = &quot;Vertical lines: medians for the year and type of report&quot;) + scale_color_brewer(labels=next_don_lab) + scale_fill_brewer(labels=next_don_lab) + xlab(&quot;&quot;) + theme(legend.title=element_blank()) + guides(fill = guide_legend(reverse = TRUE)) ) # Todo - medium: It&#39;s still not clear what is going on from year to year... maybe try animated? # Todo -- medium/high: get bars or color separation for quantiles (probability mass) within each histogram Next we plot this in two dimensions: for each individual we plot their planned current year’s donation against their reported donation for the year prior to the survey. current_planned_eas &lt;- eas_all %&gt;% select(ea_id, donation_c, donation_plan_c, year) %&gt;% # Add demographic information -- cut because it was crap #left_join(., select(eas_new_cy, all_of(demographics), ea_id, action_gwwc, start_date, end_date, income_c), by = &quot;ea_id&quot;) %&gt;% #remove likely duplicate entries (do that elsewhere too?) ; this also removes entries from years without any responses to these I guess distinct() ( current_planned_pointdensity &lt;- current_planned_eas %&gt;% # ggplot(aes(y = donation_2019, x = donation_plan_c)) + filter(year&gt;=2018) %&gt;% rowwise() %&gt;% mutate(donation_c = max(50, donation_c), donation_plan_c = max(50, donation_plan_c)) %&gt;% ungroup() %&gt;% ggplot(aes(x = donation_c, y = donation_plan_c)) + ggpointdensity::geom_pointdensity(adjust = 0.25) + geom_smooth() + do.call(scale_x_continuous, scales_point_density_min50) + do.call(scale_y_continuous, scales_point_density_min50) + scale_color_viridis_c(&quot;Neighbours&quot;) + #scale_size_continuous(&quot;Income&quot;, labels = scales::label_number_si()) + ylab(&quot;Planned (next year&#39;s) Donation&quot;) + xlab(&quot;Actual (this year&#39;s) donation&quot;) + labs(title= &quot;Last year&#39;s donation vs this year&#39;s (planned) donation&quot;, subtitle = &quot;most recent years; donations &#39;bottom-coded&#39; at 50 USD&quot;) ) #TODO: --medium importance -- facet or animate this across years&#39; The graph’s implications We repeat the above plot but only for those who report positive values for both ‘the previous year’ and for ‘planned for this year’: ( current_planned_pointdensity_no0 &lt;- current_planned_eas %&gt;% # ggplot(aes(y = donation_2019, x = donation_plan_c)) + filter(donation_c &gt;0 &amp; donation_plan_c&gt;0) %&gt;% filter(year_n-2&gt;=2018) %&gt;% rowwise() %&gt;% mutate(donation_c = max(50, donation_c), donation_plan_c = max(50, donation_plan_c)) %&gt;% ungroup() %&gt;% ggplot(aes(x = donation_c, y = donation_plan_c)) + ggpointdensity::geom_pointdensity(adjust = 0.25) + geom_smooth() + do.call(scale_x_continuous, scales_point_density_min50) + do.call(scale_y_continuous, scales_point_density_min50) + scale_color_viridis_c(&quot;Neighbours&quot;) + #scale_size_continuous(&quot;Income&quot;, labels = scales::label_number_si()) + ylab(&quot;Planned (next year&#39;s) Donation&quot;) + xlab(&quot;Actual (this year&#39;s) donation&quot;) + labs(title= &quot;Last year&#39;s vs this year&#39;s (planned) donation&quot;, subtitle = &quot;For EAs reporting a positive amount for each&quot;, caption=&quot;Last twi tears, donations &#39;bottom-coded&#39; at 50 USD&quot; ) ) #TODO: --medium importance -- facet or animate this across years&#39; Simulation-based tests (1) (Unpaired) differences in the medians and means of distributions next_current_unlinked_results_table Table 3.7: ‘Next year (plan)’ - ‘this year’ donation distributions: permutation tests Sample Statistic Point estimate CI Lower CI Upper P-value most recent years, all responses Difference in Mean 1632.8 -2425.845 5691.4 0.4 most recent years, all responses Difference in Median 437.7 307.885 567.5 0.0 The median is higher (by 438 USD point estimate). (2) Differences by individual: #report-test_rep_next_d_don_med_18_20-etc current_next_test_table Table 3.6: Planned minus last year’s donation, 2018-20, all participants who report donations Statistic Point estimate CI Lower CI Upper P-value Mean 1452 351 2553 0.012 Median 0 0 0 1.000 Discussion: … 3.9 Model of EA donation behavior Modeling ‘questions’ and approaches: We differentiate three categories of modeling: descriptive, predictive, and causal. We discuss these further [Link to methods bookdown here]. The three categories, briefly: The three categories are Descriptive (‘what relates to donation behavior’), Predictive (‘what will people with particular characteristics donate in the future’), and Causal (‘what factors actually determine the amount donated’; so if we changed these factors, donations would change). &lt;button class=“fold-button&gt;Brief: why might we care about causality here? We may care about causality because we see potential to intervene and boost those variables that cause greater giving, and/or because a better understanding of what actually drives donation behavior may yield additional insights, helping us understand the world better Brief: why might we care about prediction here? We may care about prediction because (… add brief discussion here) 3.9.1 Descriptive (and causally-suggestive) models Brief: our descriptive model ‘selection’ In our descriptive modeling we do not remove ‘insignificant’ features from the model (as in stepwise regression), nor do we shrink the coefficients towards zero (as in Ridge and Lasso models). Under the assumptions of the classical linear model and its simple extensions the coefficients we present here would be unbiased or consistent. (However, we admit that the strong assumptions of this model, particularly those embodying exogeneity, are likely to fail in important ways in the current non-experimental setting.) We retain those features of most direct interest (such as ‘how introduced to EA’) and/or theoretical importance (such as income),24 and ‘controls’ (especially time-in-EA and survey-year) that might allow us to better interpret the features of interest.25 Choosing features and modeling targets We construct several ‘feature sets’: “Key demographics, student status, and geography”, used in all models “Career/Economics”: (Income, employment status, top-6 university)26 “Pledges/commitments:” Whether ever taken a ‘Giving What We Can Pledge’, whether ‘Earning to Give’ “Controls” for age, time-in-EA, and survey-year (used in all models)27 We focus on three key outcomes: Amount donated (converted to US dollars)28 Donation as a share of income29 Whether donated more than 1000 USD # Define vector for renaming terms in regression output #&quot;new_names&#39;&quot; moved to `build/labeling_eas.R&#39; #TODO - medium -- try to move all this to build side #We impute variables where missing and normalizing all variables to be mean-zero and to be on the same scale. diqr &lt;- function(x) { (x - mean(x, na.rm=TRUE))/IQR(x, na.rm=TRUE) } gtmed &lt;- function(x) { x*(x&gt;med(x)) } #TODO -- HIGH importance -- why 43 missing values for donation_c? eas_all_s &lt;- eas_all %&gt;% filter(!is.na(don_av2_yr) &amp; year_f %in% last_3_years) %&gt;% mutate( #(re) code scaling and 2-part splits for the modeling sample (2018-20, reporting donations) age_d2sd = arm::rescale(age), #Todo (automate this with `mutate(across)` thing) age_if_older = gtmed(age), ln_age_if_older = gtmed(ln_age), ln_years_involved_post_med = gtmed(ln_years_involved), years_involved_d2sd = arm::rescale(year - as.numeric(year_involved)), years_inv_d2sd_post_med = gtmed(years_involved_d2sd), income_c_imp_diqr = diqr(income_c_imp_bc5k), age_d2sd_post_med = arm::rescale(age_if_older), income_c_imp_diqr_if_richer = gtmed(income_c_imp_diqr), ln_income_c_imp_if_richer= gtmed(ln_income_c_imp_bc5k) ) %&gt;% rowwise() %&gt;% mutate(don_share_inc_imp_bc5k = min(don_share_inc_imp_bc5k, 1)) %&gt;% #recode about 84 values so the range is between 0-1 for frac. logit to work ungroup() %&gt;% dplyr::select(all_of(c(num_out, bin_out, controls, key_demog, feat_income_employ, feat_gwwc_etg, robust_controls)), income_c, income_c_imp, income_c_imp_bc5k, income_c_imp_diqr, income_c_imp_diqr_if_richer, first_hear_ea_lump, years_involved, age, contains(&quot;d2sd&quot;), contains(&quot;iqr&quot;)) %&gt;% #I have added first_hear_ea_lump back even though we don&#39;t use it here because we want to use it in ML; I hope it doesn&#39;t mess anything up # years_involved included to put in sumstats labelled::set_variable_labels(.labels = as.list(key_eas_all_labels), .strict=FALSE) #Recode missing as 0 for all dummies, as a &#39;NA category&#39; for categoricals #also for normalized variables; i.e., set missings to the mean eas_all_s_rl &lt;- eas_all_s %&gt;% mutate(across(matches(&quot;d_|not_just_white&quot;), missing_to_zero)) eas_all_s_rl_imp &lt;- eas_all_s_rl %&gt;% mutate(across(matches(&quot;d2sd|diqr&quot;), missing_to_zero)) %&gt;% labelled::set_variable_labels(.labels = as.list(key_eas_all_labels), .strict=FALSE) #TODO: (future) -- check for sensitivity to this imputation vs dropping these obs # Write imp dataset to csv and RDS for ML/predictive modeling in donation_pred.R etc. saveRDS(eas_all_s_rl_imp, file = here(&quot;data&quot;,&quot;edited_data&quot;,&quot;eas_all_s_rl_imp.Rdata&quot;)) eas_all_s_rl_imp %&gt;% write_csv(here(&quot;data&quot;, &quot;edited_data/eas_all_s_rl_imp.csv&quot;)) count_uniq &lt;- eas_all_s_rl %&gt;% #counts of unique values for each feature in each year grp_uniq(year_f) recover_cols &lt;- count_uniq %&gt;% #any columns *without* unique features in each year? select_if(~ any(.==1))%&gt;% names() We report summary statistics on a selection of these features and target outcomes below, limited to the subset who report a zero or positive previous or current-year donation (as in our modeling). #don_inc_career_tab ( don_inc_career_tab &lt;- eas_all_s_rl_imp %&gt;% filter(!is.na(don_av2_yr)) %&gt;% mutate(`Earn-to-give` = as.factor(d_career_etg), `GwwC` = as.factor(d_gwwc_ever_0)) %&gt;% ungroup() %&gt;% filter(year_f %in% last_3_years) %&gt;% dplyr::select(starts_with(&quot;don&quot;), starts_with(&quot;d_don&quot;), starts_with(&quot;inc&quot;), -income_c_imp_diqr, d_pt_employment, d_not_employed, `Earn-to-give`, `GwwC`) %&gt;% dplyr::select(-starts_with(&quot;l_&quot;), -d_don_10pct, -ends_with(&quot;d2sd&quot;), -matches(&quot;_if_|_post_&quot;)) %&gt;% .summ(title = &quot;Donations, income, career, most recent years, (subset: reporting 0+ donation)&quot;, digits = c(0,0,1,1,2,1), labels=TRUE, logical.labels = c(&quot;No&quot;, &quot;Yes&quot;), factor.counts = FALSE, out=&quot;kable&quot;) %&gt;% kable_styling() ) Table 3.8: Donations, income, career, most recent years, (subset: reporting 0+ donation) Variable N Responses N positive Mean Sd Median 90th pct donation_c 5016 4157 8992.3 106108.5 660 10059.9 Don. ‘avg’ 5059 4528 9633.8 103967.3 1000 12000 Don./Income (imp, bc) 5015 4157 0.1 0.1 0.02 0.1 donation_plan_c 4870 4201 10625.1 107729.9 1097.69 13000 Don. &gt; 1k USD 5016 … FALSE 54% … TRUE 46% Income (not imp.) 4887 4518 83066.8 1314897.6 33956.11 130075.8 Income (imp.) 5058 5058 83540 1292509.5 36000 130000 Income (imp. bc) 5058 5058 83794.8 1292493.7 36000 130000 Employed PT 5059 474 0.1 0.3 0 0 Not Employed 5059 260 0.1 0.2 0 0 Earn-to-give 5059 … 0 70% … 1 30% GWWC (ever) 5059 … No/NA 66% … Yes 34% Above, we report donations, income, career, and GWWC pledge rates. … #TODO for future posts/time permitting: split by &#39;whether reported donation&#39;, test for differences ( demog_etc_tab &lt;- eas_all_s_rl_imp %&gt;% ungroup() %&gt;% filter(year_f %in% last_3_years) %&gt;% droplevels() %&gt;% filter(!is.na(don_av2_yr)) %&gt;% dplyr::select(-contains(&quot;don&quot;), -starts_with(&quot;d_don&quot;), -starts_with(&quot;inc&quot;), -starts_with(&quot;ln_&quot;), -d_don_10pct, -ln_income_c_imp_bc5k, -matches(&quot;d2sd|_if_|_post_&quot;)) %&gt;% select(-d_pt_employment, -d_not_employed, -d_career_etg, -d_gwwc_ever_0, -first_hear_ea_lump, -year_f, -years_involved, -age) %&gt;% select(everything()) %&gt;% labelled::set_variable_labels(.labels = as.list(key_eas_all_labels), .strict=FALSE) %&gt;% sumtable( labels = TRUE, #uses assigned in Hmisc or sjlabelled simple.kable = TRUE, title = &quot;Demography etc., most recent years (subset: reporting 0+ donation)&quot;, digits = 1, factor.counts = FALSE, out=&quot;kable&quot; ) %&gt;% kable_styling() ) Table 3.9: Demography etc., most recent years (subset: reporting 0+ donation) Variable N Percent Gender 5059 … Male 69% … Not-male 28% … Gender: No response 3% Student 5059 … Non-student 67% … Student 33% … Student: No response 0% Race/ethnicity 5059 … Just white 80% … NA 4% … Not just white 16% Where live 5059 … USA 35% … Can/Aus/NZ 12% … Country: No response 12% … EEA not Anglo 22% … Other 4% … UK/IR 15% City 5059 … Other 42% … City: No response 14% … Named big city 44% Top-6 Uni. 5059 … No 46% … NA 47% … Yes 7% ( year_etc_tab &lt;- eas_all_s_rl_imp %&gt;% ungroup() %&gt;% filter(year_f %in% last_3_years) %&gt;% droplevels() %&gt;% filter(!is.na(don_av2_yr)) %&gt;% dplyr::select(year_f, years_involved, age) %&gt;% labelled::set_variable_labels(.labels = as.list(key_eas_all_labels), .strict=FALSE) %&gt;% sumtable( labels = TRUE, #uses assigned in Hmisc or sjlabelled simple.kable = TRUE, title = &quot;Year, years-involved, age; most recent years (subset: reporting 0+ donation)&quot;, digits = 1, factor.counts = FALSE, out=&quot;kable&quot;) %&gt;% kable_styling() ) Table 3.9: Year, years-involved, age; most recent years (subset: reporting 0+ donation) Variable N Mean Std. Dev. Min Pctl. 25 Pctl. 75 Max Year of survey 5059 … 2018 38% … 2019 34% … 2020 28% Years in EA 4871 3.7 2.4 0.5 1.5 5.5 11.5 age 4430 30.2 9.8 4 24 33 102 As the table above suggests, we are modeling the most recent EA survey years … only, with roughly … shares of each year … The largest (or plurality) demographic groups (as in the sample overall, see other posts) are … Constructing models We focus on the following modeling specifications: 30 Proportional-effects ‘Quasi-Poisson’ model for ‘amount donated’ outcomes (allowing the expected donation to be an exponential function of the features). 31 Fractional logit (Papke and Wooldridge (2008)) ‘donation as a share of income’ Logit regression for the binary ‘donated over 1000 USD’ outcome # Define models (For LINEAR models ... used only in appendix, but some of these are reused in other models) #----------------------------------------------------- feat_list = list( #better to make this a &#39;named list&#39;? (TODO -- @oska would that improve the code?) c(key_demog, feat_income_employ, controls), c(key_demog, feat_income_employ, controls, robust_controls), # c(key_demog, feat_income_employ, feat_fh, controls, robust_controls), #robust controls here because &#39;first heard&#39; is likely entangled with tenure and age c(key_demog, feat_income_employ, feat_gwwc_etg, controls) ) feat_list_n = list( #better to make this a &#39;named list&#39;? (TODO -- @oska would that improve the code?) c(key_demog_n, feat_income_employ_n, controls_n), c(key_demog_n, feat_income_employ_n, controls_n, robust_controls_n), c(key_demog_n, feat_income_employ_n, feat_gwwc_etg, controls_n) ) feat_names = c(&quot;Baseline&quot;, &quot;Robust controls&quot;, &quot;Base + EtG &amp; GWWC&quot;) rhs_vars_list &lt;- rep(feat_list, length(targets_short)) #rhs_vars_list_iqr &lt;- rep(feat_list_iqr, length(targets_short)) outcome_vars_list &lt;- rep(as.list(targets_short), each=length(feat_list)) dfs &lt;- rep(list(eas_all_s_rl_imp), length(outcome_vars_list)) ## Create dataframe for modeling linear_models &lt;- make_model_df(rhs_vars_list, outcome_vars_list, dfs) # Fit linear models linear_models &lt;- linear_models %&gt;% mutate( lm_fit = fit_models( linear_models, &quot;formulas&quot;, &quot;dfs&quot;, fun = fit_lm) ) #warning `using type = &quot;numeric&quot; with a factor response will be ignored‘-’ not meaningful for factor` # @DR: Why are these models being fit on binary outcomes? DR, @OM: It is fit on all the outcomes including the binary ones, no? However, we haven&#39;t reported it yet. Anyways, I think there is still a norm of considering &#39;linear probability models&#39; in Economics, and arguments on its behalf, at least as a robustness check. # Extract coefficients, fitted and residuals model_feat_names &lt;- rep(c(feat_names), times= length(targets_short)) model_oc_names &lt;- rep(c(targets_short_names), each= length(feat_names)) model_names &lt;- paste(model_oc_names, model_feat_names, sep = &quot;: &quot;) linear_models &lt;- linear_models %&gt;% mutate(lm_coefficients = map(lm_fit, extract_coefficients, replacement_names = new_names, robust_SE = TRUE), #TODO -fix -- Medium importance (as linear is just for robustness checks...) error/warning: `&#39;^’ not meaningful for factors` lm_resids = map(lm_fit, residuals), lm_fitted = map(lm_fit, fitted)) #note: in modeling_df, lm_fit and qp_fit are the &#39;model output&#39; objects # `lm_resids` are a list of vectors of residuals from each linear model # `lm_fitted` are a list of vectors of predicted outcomes from each linear model # Error: Problem with `mutate()` column `lm_coefficients`. ## ℹ `lm_coefficients = map(...)`. ## x only 0&#39;s may be mixed with negative subscripts &lt;!-- Todo: get Oska&#39;s help to run this --&gt; #trying out some simple models just as a place to test what is going on test_qp &lt;- eas_all_s_rl_imp %&gt;% glm(don_av2_yr ~ ln_income_c_imp_bc5k + ln_age + not_male_cat + student_cat + race_cat + where_live_cat + city_cat + d_pt_employment + d_not_employed + d_top6_uni + ln_years_involved + year_f, family=quasipoisson, data =.) test_fl &lt;- eas_all_s_rl_imp %&gt;% glm(don_share_inc_imp ~ ln_income_c_imp_bc5k + ln_age + not_male_cat + student_cat + race_cat + where_live_cat + city_cat + d_pt_employment + d_not_employed + d_top6_uni + ln_years_involved + year_f, family = quasibinomial(&#39;logit&#39;), data = .) test_logit &lt;- eas_all_s_rl_imp %&gt;% glm(d_don_1k ~ age_d2sd + not_male_cat + student_cat + race_cat + where_live_cat + city_cat + income_c_imp_diqr + d_pt_employment + d_not_employed + d_top6_uni + years_involved_d2sd + year_f, family = binomial, data = .) qp_targets_short &lt;- c(&quot;don_av2_yr&quot;) qp_outcome_vars &lt;- rep(as.list(qp_targets_short), each=length(feat_list)) # List of outcome variables for quasi-poisson qp_rhs_vars &lt;- rep(feat_list, length(qp_targets_short)) # List of independent variables qp_dfs &lt;- rep(list(eas_all_s_rl_imp), length(qp_outcome_vars)) # List of dataframes to fit models to qp_models &lt;- make_model_df(qp_rhs_vars, qp_outcome_vars, qp_dfs) # Create dataframe for models # Add model names feat_group_names &lt;- c(&quot;1. Baseline&quot;, &quot;2. Robust controls&quot;, &quot;3. Base + ETG + GWWC&quot;) qp_model_names &lt;- feat_group_names qp_models &lt;- qp_models %&gt;% mutate(model_name = rep(qp_model_names, length(qp_targets_short))) # Fit quasi-poisson models qp_models &lt;- qp_models %&gt;% mutate( qp_fit = fit_models( qp_models, &quot;formulas&quot;, &quot;dfs&quot;, fun = fit_glm) ) # Extract coefficients ## Takes a little while, consider parallels package? qp_models_noexp &lt;- qp_models %&gt;% mutate(qp_coefficients = map(qp_fit, extract_coefficients, replacement_names = new_names, exponentiate = FALSE, robust_SE = TRUE), qp_resids = map(qp_fit, residuals), qp_fitted = map(qp_fit, fitted)) qp_models &lt;- qp_models %&gt;% mutate(qp_coefficients = map(qp_fit, extract_coefficients, replacement_names = new_names, exponentiate = TRUE, robust_SE = TRUE), qp_resids = map(qp_fit, residuals), qp_fitted = map(qp_fit, fitted)) #Note: redone/redoing - fractional logit instead of Quasi-poisson with offset #Discussion: fl_targets_short &lt;- c(&quot;don_share_inc_imp_bc5k&quot;) fl_outcome_vars &lt;- rep(as.list(fl_targets_short), each=length(feat_list)) # List of outcome variables for quasi-poisson fl_rhs_vars &lt;- rep(feat_list, length(fl_targets_short)) # List of independent variables fl_dfs &lt;- rep(list(eas_all_s_rl_imp), length(fl_outcome_vars)) # List of dataframes to fit models to #---------- # Function to remove a particular string from a list #remove_str_list (moved to rstuff functions) # Create dataframe for models fl_models &lt;- make_model_df(fl_rhs_vars, fl_outcome_vars, fl_dfs) fl_models &lt;- fl_models %&gt;% mutate(model_name = rep(feat_group_names, length(fl_targets_short))) # Fit fractional logit models fl_models &lt;- fl_models %&gt;% mutate( fl_fit = fit_models( fl_models, &quot;formulas&quot;, &quot;dfs&quot;, fun = fit_glm, family = quasibinomial(&#39;logit&#39;)) ) # Extract coefficients ## Takes a little while, consider parallels package? fl_models_noexp &lt;- fl_models %&gt;% mutate(fl_coefficients = map(fl_fit, extract_coefficients, replacement_names = new_names, exponentiate = FALSE, robust_SE = TRUE), fl_resids = map(fl_fit, residuals), fl_fitted = map(fl_fit, fitted)) fl_models_noexp_nonrobust &lt;- fl_models %&gt;% mutate(fl_coefficients = map(fl_fit, extract_coefficients, replacement_names = new_names, exponentiate = FALSE, robust_SE = FALSE), fl_resids = map(fl_fit, residuals), fl_fitted = map(fl_fit, fitted)) fl_models &lt;- fl_models %&gt;% mutate(fl_coefficients = map(fl_fit, extract_coefficients, replacement_names = new_names, exponentiate = TRUE, robust_SE = TRUE), fl_resids = map(fl_fit, residuals), fl_fitted = map(fl_fit, fitted)) Models (tables and plots of results; latest years combined) We put together forest plots of (normalized) coefficients from the distinct set of models outlined above, where these can be compared on the same scales. Specifically, we consider, for each of the three key outcomes (‘amount donated (averaged)’, ‘donation as a share of income’, ‘donated over 1000 USD’), models with three specific sets of features, yielding nine models in total (plus robustness checks in the appendix). The feature sets, which we will refer to in the forest plots below, are: 1. “Base” (baseline model) Demographics:32 Log age, Gender, Student, Race/ethnicity, Where live, City Career-related: Employed PT, Not Employed, Top-6 Uni. Controls: Years in EA (log), Year of survey Logit model: standardizations For the Logit models we use standardizations instead of logged continuous variables. We divide age and tenure by two sample standard deviations for each, following Gelman (2008). This allows the coefficients of continuous features like income to be compared to those for binary features like “whether employed”. We divide income by the sample inter-quartile range (Error in h(simpleError(msg, call)) : error in evaluating the argument ‘x’ in selecting a method for function ‘format’: missing values and NaN’s not allowed if ‘na.rm’ is FALSE USD). As the distribution appears highly skewed, normalizing income by 2sd would yield extremely large and hard-to-interpret coefficients. 2. “Robust controls”: Including all of the features in Base as well as a second term for each of “Years in EA (log), Log age” that takes a positive value only where these exceed their respective sample medians, and is otherwise set to zero. These represent ‘adjustment terms’ allowing us to see whether and how time-in-EA, age, and income may have a different relationship with donations at higher values of each of these.33 3. “Base + ETG + GWWC”: Including all of the features in Base as well as the binary variables “GWWC (ever), EtG”, i.e., whether reported ever having taken the Giving What We Can Pledge, and whether they report their career as ‘earning-to-give’. Note that we report models with each of the three feature sets in each of the forest plots below. However, each forest plot reports on a single outcome and a single ‘theme’, e.g., focusing on reporting just the coefficients on demographics from across each of the above three model feature sets (with some repeated coefficients across plots). These themes are Demographics (including age and time-in-EA)* Employment/career, GWWC, EtG, Income** “Non-response” to particular questions (in the appendix)*** However, it is important to remember that the reported estimates in each forest plot come from models that ‘control for’ other features (as reported). Note that we exclude the ‘two-part’ coefficients (in the ‘Robust controls’ models) from the forest plots.34 We present the coefficients on ‘Age, Time in EA, Income, and nonlinear adjustments for each of these’ in a separate set of tables (web link).35, 36 # Create variable groupings for displaying coefficients ## Can&#39;t think of a tidier way to do this... ### Check that this extracts all necessary demog_coefs &lt;- c(&quot;Age&quot;, &quot;Where live:&quot;, &quot;Student&quot;, &quot;city&quot;, &quot;Gender:&quot;, &quot;white&quot;, &quot;race&quot;) emp_gw_etg_coefs &lt;- c(&quot;top-6 uni&quot;, &quot;employ&quot;, &quot;etg&quot;, &quot;gwwc&quot;) #fh_coefs &lt;- c(&quot;hear&quot;) nr_coefs &lt;- c(&quot;response&quot;, &quot;NA&quot;) nonlin_coefs &lt;- c(&quot;income&quot;, &quot;age&quot;, &quot;year&quot;) #coefficients of interest not (always) reported elsewhere, allowing us to consider nonlinearity # Filter the coefficients returned from using broom::tidy ## Keep only those specified in character vector keep extract_coefs &lt;- function(df, keep, term_col = term, ignore.case = TRUE, exclude = NULL){ # Add assertion statements in/doc string if (ignore.case == TRUE){ keep &lt;- tolower(keep) } keep &lt;- paste(keep, collapse=&quot;|&quot;) coef_df &lt;- df %&gt;% filter(str_detect(tolower({{term_col}}), keep)) if (!is.null(exclude)){ exclude &lt;- paste(exclude, collapse=&quot;|&quot;) coef_df &lt;- coef_df %&gt;% filter(!str_detect(tolower({{term_col}}), exclude)) } return(coef_df) } # Extract coefficients for each feature set, for making forest plots (and tables) # forms &lt;- list(&quot;qp&quot;, &quot;fl&quot;, &quot;logit&quot;) #--&gt; pmap (todo @oska) qp_coefs &lt;- qp_models %&gt;% select(qp_coefficients, model_name, outcome) %&gt;% tidyr::unnest(., cols = c(qp_coefficients)) qp_coefs_noexp &lt;- qp_models_noexp %&gt;% select(qp_coefficients, model_name, outcome) %&gt;% tidyr::unnest(., cols = c(qp_coefficients)) fl_coefs &lt;- fl_models %&gt;% select(fl_coefficients, model_name, outcome) %&gt;% tidyr::unnest(., cols = c(fl_coefficients)) fl_coefs_noexp &lt;- fl_models_noexp %&gt;% select(fl_coefficients, model_name, outcome) %&gt;% tidyr::unnest(., cols = c(fl_coefficients)) fl_coefs_noexp_nonrobust &lt;- fl_models_noexp_nonrobust %&gt;% select(fl_coefficients, model_name, outcome) %&gt;% tidyr::unnest(., cols = c(fl_coefficients)) logit_coefs &lt;- logit_models %&gt;% select(logit_coefficients, model_name, outcome) %&gt;% tidyr::unnest(., cols = c(logit_coefficients)) # feature_sets &lt;- list(&quot;demog&quot;, &quot;inc_emp_gw_etg&quot;, &quot;fh&quot;, &quot;nr&quot;) #--&gt; pmap2 feature_sets, forms (todo @oska) .. or maybe map across forms but not feature sets here, as we need bespoke exclusions exclude_demog_coefs &lt;- c(&quot;response&quot;,&quot;older&quot;, &quot;post_med&quot;, &quot;ln_&quot; ) exclude_inc_coefs &lt;- c(&quot;response&quot;, &quot;if above&quot;, &quot;older&quot;, &quot;na&quot;) #exclude_fh_coefs &lt;- c(&quot;response&quot;, &quot;if above&quot;, &quot;older&quot;) #will prob need to add coefs for all the small fh categories if we put these all in demog_coefs_qp &lt;- extract_coefs(qp_coefs, demog_coefs, exclude = c(exclude_demog_coefs)) emp_gw_etg_coefs_qp &lt;- extract_coefs(qp_coefs, emp_gw_etg_coefs, exclude = exclude_inc_coefs) nr_coefs_qp &lt;- extract_coefs(qp_coefs, nr_coefs) nonlin_coefs_qp &lt;- extract_coefs(qp_coefs, nonlin_coefs) nonlin_coefs_qp_noexp &lt;- extract_coefs(qp_coefs_noexp, nonlin_coefs) nonlin_coefs_qp_combo &lt;- bind_rows(nonlin_coefs_qp_noexp %&gt;% filter(str_detect(term, &quot;Year of&quot;)==FALSE), nonlin_coefs_qp %&gt;% filter(str_detect(term, &quot;Year of&quot;)==TRUE)) demog_coefs_fl &lt;- extract_coefs(fl_coefs, demog_coefs, exclude = exclude_demog_coefs) emp_gw_etg_coefs_fl &lt;- extract_coefs(fl_coefs, emp_gw_etg_coefs, exclude = exclude_inc_coefs) #TODO: doublecheck the income coefficients: nr_coefs_fl &lt;- extract_coefs(fl_coefs, nr_coefs) nonlin_coefs_fl &lt;- extract_coefs(fl_coefs, nonlin_coefs) nonlin_coefs_fl_noexp &lt;- extract_coefs(fl_coefs_noexp, nonlin_coefs) nonlin_coefs_fl_noexp_nonrobust &lt;- extract_coefs(fl_coefs_noexp_nonrobust, nonlin_coefs) nonlin_coefs_fl_combo &lt;- bind_rows(nonlin_coefs_fl_noexp %&gt;% filter(str_detect(term, &quot;Year of&quot;)==FALSE), nonlin_coefs_fl %&gt;% filter(str_detect(term, &quot;Year of&quot;)==TRUE)) demog_coefs_logit &lt;- extract_coefs(logit_coefs, demog_coefs, exclude = exclude_demog_coefs) emp_gw_etg_coefs_logit &lt;- extract_coefs(logit_coefs, c(&quot;income&quot;, emp_gw_etg_coefs), exclude = exclude_inc_coefs) nr_coefs_logit &lt;- extract_coefs(logit_coefs, nr_coefs) nonlin_coefs_logit &lt;- extract_coefs(logit_coefs, nonlin_coefs) group_fp_do &lt;- function(df, groups=model_name, xlims=c(NA,NA), vl=1){ df %&gt;% grouped_forest_plot(., groups = {{groups}}, vline = {{vl}}) + coord_cartesian(xlim = {{xlims}}) + scale_colour_discrete(name = &quot;&quot;, labels = function(x) str_wrap(x, width = 15)) + theme_bw() + theme(legend.position = &quot;bottom&quot;) + guides(fill=guide_legend(nrow=2,byrow=TRUE)) } #TODO: Add to caption at bottom, automate, specify the exact models (or at least explain the control variables) # Legend on top or bottom? # Todo -- gray line in background # add more grid lines fp_qp_subtitle &lt;- &quot;Quasi-poisson: relative rates, 95% CIs (Robust SEs); colors = models&quot; fp_logit_subtitle &lt;- &quot;Logit model, proportional effects, 95% CIs; colors = models&quot; fp_fl_subtitle &lt;- &quot;Frac. Logit model, prop. effects, 95% CIs; colors = models&quot; fp_caption = str_wrap(&quot;Results come from three distinct models, each with different sets of features. These models and their features are fully described in the main text. All confidence intervals are heteroskedasticity-robust (White, 1980).&quot;, 120) #todo (@oska) purr::map these too fp_demog_coefs_qp &lt;- demog_coefs_qp %&gt;% group_fp_do(vl=1) + labs(title= &quot;&#39;Donation amount&#39;, demog. coefficients&quot;, subtitle = fp_logit_subtitle, caption = fp_caption ) + theme(plot.caption = element_text(size = 8, hjust = 0)) fp_demog_coefs_fl &lt;- demog_coefs_fl %&gt;% group_fp_do(vl=1) + labs(title= &quot;&#39;Donation/income&#39;, demog. coefficients, 2018-20&quot;, subtitle = fp_fl_subtitle, caption = fp_caption) #TODO -- get me my ticks for 0.25, 0.5 etc fp_demog_coefs_logit &lt;- demog_coefs_logit %&gt;% group_fp_do(vl=1) + labs(title= &quot;&#39;Donated $1000+&#39;, demog. coef., 2018-20&quot;, subtitle = fp_logit_subtitle, caption = fp_caption) #todo (@oska) purr::map these too fp_emp_gw_etg_coefs_qp &lt;- emp_gw_etg_coefs_qp %&gt;% filter(!(str_detect(model_name, &quot;Robust&quot;) &amp; str_detect(term, &quot;Income&quot;))) %&gt;% #Don&#39;t show income in models with nonlinear income terms group_fp_do(vl=1) + labs(title= &quot;&#39;Donation amount&#39;, career-related coefs, 2018-20&quot;, subtitle = fp_qp_subtitle, caption = fp_caption) #TODO -- NA STILL HERE! fp_emp_gw_etg_coefs_fl &lt;- emp_gw_etg_coefs_fl %&gt;% filter(!(str_detect(model_name, &quot;Robust&quot;) &amp; str_detect(term, &quot;Income&quot;))) %&gt;% group_fp_do(vl=1) + labs(title= &quot;&#39;Donation as share of income&#39;, proportional model, career-related coefs&quot;, subtitle = fp_qp_subtitle, caption = fp_caption) fp_emp_gw_etg_coefs_logit &lt;- emp_gw_etg_coefs_logit %&gt;% filter(!(str_detect(model_name, &quot;Robust&quot;) &amp; str_detect(term, &quot;Income&quot;))) %&gt;% filter(!(str_detect(term, &quot;NA&quot;))) %&gt;% group_fp_do(vl=1, xlims=c(0,NA)) + labs(title= &quot;&#39;Donated $1000+&#39;, Logit model, career-related coefs&quot;, subtitle = fp_logit_subtitle, caption = c(fp_caption) ) ### ##TODO -- get NA out of here! Model theme: Demographics Below, we plot the estimates and (heteroskedasticity-robust) 95% confidence intervals for key demographic coefficients for each of the three models.37 We first present the results from our Quasi-Poisson model of donation amount (expressed as the average of current and planned donation in a year, or whichever is noted). This model allows effects to be “proportional”, as described and interpreted below.38 fp_demog_coefs_qp In the figure above, the vertical bar at “1” represents the coefficient of “no difference in donation between these groups, all else equal”. … (possible brief discussion of key results here, or only in Forum post). fp_demog_coefs_fl Above we plot the estimates, for the same features, for our (Fractional Logit) models of the ‘share of income donated’.39 … rpct &lt;- function(x){ (round(x, 3) -1)*100} … EAs living in named cities donate … a roughly 6.7% greater share in our baseline model.40 Non-males seem to be donating a … 89.3% as high a share in our baseline model. Students also seem to donate … 93.9% as high in our baseline model. fp_demog_coefs_logit The final outcome variable is ‘whether an individual donated $1000 or more’. 41 Age has a … a two standard deviation increase in age (about 19.6 years older) is associated with a near-doubling of the probability of donating 1000 USD or more in our model. (1.85 times as high in our baseline model.) Again, in our baseline model… EAs living in named big cities are … roughly 17% more likely, Non-males are about 80.4% as likely as males, ‘Not-just-white’ people about 81.9% as likely as white people, and students 64.9% as likely as nonstudents… …to donate at least 1000 dollars, all else equal. The graph suggests that each of these results are … Model theme: Employment/career, GwwC, EtG, (income) Next we consider employment and career-related features.42 fp_emp_gw_etg_coefs_qp … GWWC pledge Those who attended global top-6 universities (defined above) donate about 31.9% more in our baseline model point estimate. … fp_emp_gw_etg_coefs_fl … Discussions: patterns for shares of income, key relationships… fp_emp_gw_etg_coefs_logit {atterns for our logit models of “whether donated at least 1000 USD” … (compare across models) Here (for models without the two-part controls) we also present the relationship to income. An income that is 56787 USD greater (the 25-75 ‘interquartile range’) is associated with a 2.86 times greater probability donating 1k or more. … (other standouts) Age, time-in-EA, Income, Year; possible nonlinearity #Making table of *raw coefficients* for logged income variables here, as these have elasticity interpretations nonlin_tables &lt;- bind_rows(nonlin_coefs_qp_combo, nonlin_coefs_fl_combo, nonlin_coefs_logit) %&gt;% select(outcome, term, model_name, estimate, std.error, p.value, conf.low, conf.high) %&gt;% mutate( term = str_replace_all(term, c(&quot;ln_&quot; = &quot;log &quot;, &quot;EApost_med&quot; = &quot;EA (post median)&quot;)), term = str_replace_all(term, key_eas_all_labels), term = str_replace_all(term, c(&quot;Involved&quot; = &quot;in&quot;)), term = str_replace_all(term, c(&quot;_bc5k&quot; = &quot;&quot;)), term = str_replace_all(term, c(&quot;imp.&quot; = &quot;imp., bc&quot;)), term = str_replace_all(term, c(&quot;years_inv_d2spost_med&quot; = &quot;Years in EA (2sd norm) post-median&quot;)) ) %&gt;% filter(!str_detect(model_name, &quot;GWWC&quot;)) %&gt;% mutate( outcome = str_replace_all(outcome, key_eas_all_labels ) ) %&gt;% arrange(term) %&gt;% group_by(outcome) %&gt;% group_split nonlin_don_avg_tab &lt;- nonlin_tables[[1]] %&gt;% select(-outcome) %&gt;% .kable(caption = nonlin_tables[[1]][[1,&quot;outcome&quot;]], col.names = NA, digits=3) %&gt;% .kable_styling(&quot;striped&quot;) nonlin_don_gt1k_tab &lt;- nonlin_tables[[2]] %&gt;% select(-outcome) %&gt;% .kable(caption = nonlin_tables[[2]][[1,&quot;outcome&quot;]], col.names = NA, digits=3) %&gt;% .kable_styling(&quot;striped&quot;) nonlin_donshare_tab &lt;- nonlin_tables[[3]] %&gt;% select(-outcome) %&gt;% .kable(caption = nonlin_tables[[3]][[1,&quot;outcome&quot;]], col.names = NA, digits=3) %&gt;% .kable_styling(&quot;striped&quot;) Discussion of modeling continuously-valued features We next present the estimates, from six of the nine models (excluding models involving GWWC and earning-to-give), for the continuous-valued features (age, income, and years-in-EA) as well for the survey-year categorical feature. We present these for both the baseline and the ‘Robust control’ models; the latter allow us to consider distinct patterns where values are above-median. We present these in tables rather than forest plots, as the interpretation is subtle. nonlin_don_avg_tab Table 3.10: Don. ‘avg’ term model_name estimate std.error p.value conf.low conf.high log age Baseline 0.506 0.327 0.123 -0.136 1.147 log age Robust controls 0.372 0.392 0.343 -0.397 1.140 log Age (if age &gt; med.) Robust controls 0.033 0.049 0.504 -0.064 0.129 Log income (imp., bc) Baseline 0.951 0.095 0.000 0.765 1.138 Log income (imp., bc) Robust controls 0.929 0.115 0.000 0.704 1.154 Log Income (imp., bc) (if above med.) Robust controls 0.013 0.020 0.509 -0.026 0.053 log Years in EA Baseline 0.627 0.099 0.000 0.434 0.821 log Years in EA Robust controls 0.526 0.141 0.000 0.251 0.802 log Years in EA (post median) Robust controls 0.076 0.114 0.505 -0.147 0.299 Year of survey: 2019 Baseline 1.199 0.357 0.611 0.596 2.411 Year of survey: 2019 Robust controls 1.176 0.351 0.644 0.591 2.338 Year of survey: 2020 Baseline 1.208 0.382 0.620 0.572 2.554 Year of survey: 2020 Robust controls 1.176 0.380 0.670 0.558 2.476 In the Baseline model… we see that, unsurprisingly, donations clearly and strongly increase in income, all else equal; in the parlance of Economics, donation seems to be a ‘Normal good.’ The estimated income elasticity is 0.951, suggesting that, all else equal, on average, as income increases in our data by some share ‘X’, donations increase by slightly … than this share. In model 2 we include an adjustment coefficient to allow nonlinearity, allowing the elasticity of donations in income to be distinct for income levels above the median. … Age: the baseline coefficient suggests that as age doubles, contributions increase by 50.6% on average, all else equal. … Time-in-EA is strongly related to donations, even ‘controlling for age, etc.’ (and vice versa). A doubling of years in EA is associated with a 62.7% greater donation. The nonlinear adjustment term … Each of the ‘survey year difference’ estimates (for 2019 and 2020, with 2018 as the base year) are (… update/doublecheck) fairly close to 1 (representing ‘no difference’). However, 95% confidence intervals are rather … wide, suggesting a lack of statistical power to discern a difference. … We next present the corresponding coefficients for our fractional logit models of ‘donations as a share of income’. nonlin_donshare_tab Table 3.10: Don./Income (imp, bc) term model_name estimate std.error p.value conf.low conf.high log age Baseline 0.819 0.146 0.000 0.533 1.105 log age Robust controls 0.877 0.192 0.000 0.500 1.254 log Age (if age &gt; med.) Robust controls -0.012 0.025 0.622 -0.061 0.036 Log income (imp., bc) Baseline 0.045 0.042 0.289 -0.038 0.128 Log income (imp., bc) Robust controls 0.017 0.058 0.766 -0.097 0.132 Log Income (imp., bc) (if above med.) Robust controls 0.008 0.009 0.389 -0.010 0.026 log Years in EA Baseline 0.597 0.048 0.000 0.504 0.691 log Years in EA Robust controls 0.654 0.082 0.000 0.494 0.814 log Years in EA (post median) Robust controls -0.044 0.056 0.427 -0.153 0.065 Year of survey: 2019 Baseline 0.713 0.125 0.007 0.558 0.912 Year of survey: 2019 Robust controls 0.719 0.125 0.008 0.562 0.919 Year of survey: 2020 Baseline 0.694 0.136 0.007 0.532 0.905 Year of survey: 2020 Robust controls 0.701 0.136 0.009 0.538 0.915 The baseline models suggest that donation as a share of income is roughly (… constant, or slightly increasing in income. For every doubling of income, the share of income donated is seen to increase by 4.5%. … Discussion (or forum only): age vs donation share… Finally, we consider the models of ‘donated 1k or more’: nonlin_don_gt1k_tab Table 3.10: Don. &gt; 1k USD term model_name estimate std.error p.value conf.low conf.high Age (2sd norm.) Baseline 1.846 0.087 0.000 1.556 2.191 Age (2sd norm.) Robust controls 1.761 0.153 0.000 1.305 2.376 Age if older (norm.) Robust controls 1.063 0.153 0.690 0.787 1.436 Income (imp., bc, IQR norm) Baseline 2.860 0.130 0.000 2.217 3.691 Income (imp., bc, IQR norm) Robust controls 4.535 0.075 0.000 3.918 5.248 Income (imp., bc, IQR norm)_if_richer Robust controls 0.374 0.125 0.000 0.293 0.477 Year of survey: 2019 Baseline 0.730 0.134 0.019 0.561 0.949 Year of survey: 2019 Robust controls 0.747 0.138 0.034 0.570 0.978 Year of survey: 2020 Baseline 0.625 0.141 0.001 0.474 0.825 Year of survey: 2020 Robust controls 0.652 0.146 0.003 0.489 0.869 Years in EA (2 sd norm.) Baseline 2.427 0.077 0.000 2.086 2.823 Years in EA (2 sd norm.) Robust controls 8.111 0.184 0.000 5.651 11.644 Years in EA (2sd norm) post-median Robust controls 0.149 0.266 0.000 0.089 0.251 Here we present exponentiated coefficients, representing relative proportional rates of this outcome for the distinct groups. Age is … Year coefficients 43 The ‘years in EA’ coefficients are (…) extremely strong. These suggest that a 2 sd increase in tenure in EA (4.89 years) is associated with a 2.43 times greater relative chance of donating 1k or more relative to the base group), all else equal. 3.9.2 Predictive models We use elastic-net and random-forest modeling approaches with validation (these are standard in the modern ‘machine learning’ tooklit), to derive a model that ‘predicts well’.* Discussion: why build ‘predictive models of donation’? TODO: move (some?) of this to methods section We focus on predicting the individual’s donation in a year, focusing on the same set of outcomes used in the previous section. For this model to be useful for an actual prediction problem going forward, it would need to rely on ‘ex-ante’ characteristic that were already observable at the time of a career/EtG/pledge decision.44 These might include immutable demographics, career plans, and pledges previously taken, and consider year and trend effects. Although we have these models in mind, this is not what we are doing here. We are not posing a specific ‘prediction problem’ per se. Instead we are using machine learning tools built for prediction problems to generate ‘data-driven insights’ about factors related to EA donation behavior. Here, we do not than directly specifying all of the included components of the model (features, interaction terms, etc.). Instead we provide a large set of possible ‘inputs’ and use ML techniques to train models that should predict well outside of the data they are trained on. These models should do a good job of accomplishing the task: ‘if you gave me a set of features of an EA, I should have a fairly accurate guess at what they will donate.’ The insights from these models should also be treated with caution. Again, they may not be deriving causal relationships. Furthermore, the parameters derived from model-fitting ML procedures are not in general unbiased or consistent, and it is difficult to derive proper confidence intervals for these parameters. Still, the benefit of this exercise may be considered ‘the derivation of robust and predictive relationships in the data that are mainly driven by the data itself, rather than our preconcieved ideas.’ These models may also be useful building blocks towards future predictive work. Discussion: ‘elastic net’ models TODO: move (some?) of this to methods section In brief, the elastic net models involve linear models (log-linear in our case), i.e., ‘regressions’, that carefully ‘penalize’ the (squared) magnitude of coefficients, in effect shrinking these towards zero. The penalties are specifically ‘tuned’ and ‘validated’ to maximize the predictive power of the model. As these are essentially regression approaches, we can report the sign and magnitude of the coefficients used in the ‘optimally tuned’ predictive model. (However, we should be careful about interpreting these parameters, and statistical inference is challenging. See e.g., Mullainathan and Spiess (2017a) for a detailed discussion.) Discussion: ‘Decision tree’ models TODO: move (some?) of this to methods section Decision tree models (which we do not report on here) take a different approach, attempting to discern optimal ways to split the data into conditional groups (e.g., ‘income over 20k’) and subgroups (e.g., ‘students versus nonstudents with income below 20k’), and finally making a prediction for each subgroup at the ‘bottom of the tree’. The random forest approach extends the above to allow a sort of averaging across an ensemble of trees that are derived independently, each selecting a random subset of the features. We fit these models/approaches, starting with a wide set of potential features, to explain each of the three main outcomes considered above.45 Below, we plot the seven most ‘important’ features (aka variables) for predicting the log of donation amount (average of planned and actual, where available) according to the random forest and elastic net (‘regression’) models. # Need to see each of these variables for all models tuning_folder &lt;- here(&quot;analysis&quot;, &quot;intermed_results&quot;, &quot;donation_prediction&quot;, &quot;tuning_results&quot;) final_models &lt;- here(&quot;analysis&quot;, &quot;intermed_results&quot;, &quot;donation_prediction&quot;, &quot;final_models&quot;) l_don_av_2yr_best_params &lt;- readRDS(here(final_models, &quot;l_don_av_2yr.Rdata&quot;)) %&gt;% filter(!grepl(&quot;decision&quot;, model, ignore.case = TRUE)) don_share_inc_imp_best_params &lt;- readRDS(here(final_models, &quot;don_share_inc_imp.Rdata&quot;)) %&gt;% filter(!grepl(&quot;decision&quot;, model, ignore.case = TRUE)) d_don_1k_best_params &lt;- readRDS(here(final_models, &quot;d_don_1k.Rdata&quot;)) %&gt;% filter(!grepl(&quot;decision&quot;, model, ignore.case = TRUE)) l_don_av_2yr_best_params_filter &lt;- readRDS(here(final_models, &quot;l_don_av_2yr_filter.Rdata&quot;)) %&gt;% filter(!grepl(&quot;decision&quot;, model, ignore.case = TRUE)) # don_share_inc_imp_best_params_filter &lt;- readRDS(here(final_models, &quot;don_share_inc_imp_filter.Rdata&quot;)) # d_don_1k_best_params_filter &lt;- readRDS(here(final_models, &quot;d_don_1k_filter.Rdata&quot;)) l_don_av_2yr_best_params &lt;- l_don_av_2yr_best_params %&gt;% bind_rows(l_don_av_2yr_best_params_filter) #don_share_inc_imp_best_params &lt;- don_share_inc_imp_best_params %&gt;% bind_rows(don_share_inc_imp_best_params_filter) #d_don_1k_best_params &lt;- d_don_1k_best_params %&gt;% bind_rows(d_don_1k_best_params_filter) recode_params &lt;- function(df){ # Shortcut function to tidy up variable names in parameter df df &lt;- df %&gt;% dplyr::select(model, vi) %&gt;% tidyr::unnest(vi) %&gt;% mutate(model = str_replace_all(model, c(&quot;preprocess_&quot; = &quot;&quot;, &quot;_&quot; = &quot; &quot;)), Variable = str_replace_all(Variable, key_eas_all_labels), Variable = str_replace_all(Variable, c(&quot;_&quot; = &quot; &quot;, &quot;_Student&quot; =&quot;&quot;, &quot;ln&quot; = &quot;log&quot;)), Sign = if_else(is.na(Sign), &quot;NA&quot;, Sign)) } norm_vi &lt;- function(df, slice_top = 7){ # Shortcut function for calculating normalized variable importance # Not reproducible... df %&gt;% group_by(model) %&gt;% mutate(Norm = scale_var(Importance)) %&gt;% group_by(Variable) %&gt;% mutate(Total_Norm = sum(Norm)) %&gt;% group_by(model) %&gt;% slice_max(Total_Norm, n = slice_top) %&gt;% mutate(Variable = fct_reorder(Variable, Norm)) } plot_vi &lt;- function(df, shapes = shape_colours){ # Shortcut function for plotting normalized variable importance (output of norm_vi) df %&gt;% ggplot(aes(y = Variable, x = Norm, colour = model, shape = Sign)) + scale_shape_manual(values = shapes) + geom_point(size = 4, stroke = 5) + xlab(&quot;Normalised feature importance&quot;) + ylab(&quot;&quot;) } #specific changing of variable and signs for the below. mutate_labels_sign_snip &lt;- function(df) { df %&gt;% mutate( Variable = str_replace_all(Variable, c(&quot;First-heard EA&quot;=&quot;Heard EA:&quot;, &quot;response&quot; = &quot;resp.&quot;, &quot;Gender Gender&quot; = &quot;Gender&quot;, &quot;unknown&quot; = &quot;No resp.&quot;, &quot;Student Student&quot; = &quot;Student&quot;, &quot;X80000&quot; = &quot;80000&quot;)), Sign = if_else(is.na(Sign), &quot;NA&quot;, Sign) ) } # Set colors for shapes as a named vector shape_colours &lt;- c(&quot;NA&quot; = 120, &quot;NEG&quot; = 95, &quot;POS&quot; = 43) # Tidy up parameters l_don_av_2yr_best_params_recode &lt;- l_don_av_2yr_best_params %&gt;% filter(is.na(filter_name)) %&gt;% recode_params l_don_av_2yr_best_params_recode_filter &lt;- l_don_av_2yr_best_params %&gt;% filter(!is.na(filter_name)) %&gt;% recode_params don_share_inc_imp_best_params_recode &lt;- don_share_inc_imp_best_params %&gt;% recode_params d_don_1k_best_params_recode &lt;- d_don_1k_best_params %&gt;% recode_params #NOT RUN # Starting to plot decision trees (early stages) l_don_av_2yr_tree &lt;- l_don_av_2yr_best_params %&gt;% mutate( model = str_replace_all( model, c(&quot;preprocess_&quot; = &quot;&quot;, &quot;_&quot; = &quot; &quot;)) ) %&gt;% filter(!grepl(&quot;decision&quot;, model, ignore.case = TRUE)) ) %$% workflowsets::extract_fit_parsnip(fit[[1]]) # l_don_av_2yr_treeX &lt;- l_don_av_2yr_best_params %&gt;% # mutate( # model = str_replace_all( # model, c(&quot;preprocess_&quot; = &quot;&quot;, &quot;_&quot; = &quot; &quot;)) # ) %&gt;% # filter(str_det(model, &quot;decision tree&quot;)) %$% # workflowsets::extract_fit_parsnip(fit[[1]]) #, fig.dim = c(10, 10) ( iplot_l_don_av_2yr_best_params &lt;- l_don_av_2yr_best_params_recode %&gt;% filter(!grepl(&quot;tree&quot;, model, ignore.case = TRUE)) %&gt;% norm_vi(slice_top = 10) %&gt;% mutate_labels_sign_snip %&gt;% slice_max(Total_Norm, n = 10) %&gt;% mutate(Variable = fct_reorder(Variable, Norm)) %&gt;% ggplot(aes(y = Variable, x = Norm, colour = model, shape = Sign)) + scale_shape_manual(values = c(120, 95, 43)) + geom_point( position = position_jitter(seed = 42, width = 0.1, height = 0.1), size = 4, stroke = 5) + xlab(&quot;Normalised feature importance&quot;) + ylab(element_blank()) + ggtitle(&quot;Normalized importance scores: predicting log(don.)&quot;) ) Above, we report the ‘importance scores’ for the ten most important features (‘variables’) for two distinct approaches to predicting log (average) donation.46 These importance scores are technically defined here. For the elastic net (“linear reg”) approach, we depict the coefficients’ signs with a “+” or “-”; for tree/forest-based modeling this is less straightforward. Most important predictors… The ‘non-response’ features As they are difficult to interpret, and probably not useful for future predictions, we will basically not discuss the non-response features further.47 As our data exhibited some large donation-amount outliers, (which is naturally tied to high income), we re-trained models on a subset of the data with only respondents who earned less than $500,000, with importance scores reported below. ( iplot_l_don_av_2yr_best_params_filter &lt;- l_don_av_2yr_best_params_recode_filter %&gt;% filter(!grepl(&quot;tree&quot;, model, ignore.case = TRUE)) %&gt;% norm_vi(slice_top = 10) %&gt;% mutate_labels_sign_snip %&gt;% slice_max(Total_Norm, n = 10) %&gt;% mutate(Variable = fct_reorder(Variable, Norm)) %&gt;% ggplot(aes(y = Variable, x = Norm, colour = model, shape = Sign)) + scale_shape_manual(values = c(120, 95, 43)) + geom_point( position = position_jitter(seed = 42, width = 0.1, height = 0.1), size = 4, stroke = 5) + xlab(&quot;Normalised feature importance&quot;) + ggtitle(&quot;Importance: predict log(don.) (filter: income &lt; 500k)&quot;) ) The results are … We next focus specifically on the elastic-net regression-based model. # TODO -- important -- what are the base groups here, especially for &#39;first-heard&#39;? I thought in previous work we made &quot;don&#39;t remember&quot; the base group! This is important for interpreting the coefficient signs! # Plot all coefficients for elastic net model #TODO (DR @ OF -- plot the coefficients rather than the importance weights? (the latter are absolute value t-values anyways)) #TODO/check DR @ OF -- why is &#39;year of survey 2015&#39; (and 2017) in here? those years should have been removed from the dataset; I think they have been, but still it somehow reports an importance score? -- OK I am removing &#39;0&#39; importance scores for now ( enet_coefs_ldon &lt;- l_don_av_2yr_best_params_recode %&gt;% filter( grepl(&quot;regression&quot;, model, ignore.case = TRUE) &amp; Importance!=0) %&gt;% mutate_labels_sign_snip %&gt;% #mutate(Norm = scale_var(Importance)) %&gt;% mutate(Variable = fct_reorder(Variable, Importance)) %&gt;% ggplot(aes(y = Variable, x = Importance, shape = Sign)) + scale_shape_manual(values = c(95, 43)) + geom_point(size = 2, stroke = 4) + xlab(&quot;Feature importance&quot;) + ggtitle(&quot;Importance scores: predicting log(don.)&quot;) ) The graph above presents the overall ranking of importance scores within the elastic-net linear regression model, with symbols depicting whether these features take on a positive or negative sign. Results… Predictive model: Shares of income donated Next we consider the shares-of-income donated48 ( iplot_don_share_inc_imp_best_params &lt;- don_share_inc_imp_best_params_recode %&gt;% filter(!grepl(&quot;tree&quot;, model, ignore.case = TRUE)) %&gt;% mutate_labels_sign_snip %&gt;% norm_vi(slice_top = 10) %&gt;% plot_vi() + ggtitle(&quot;Importance scores: predicting share of income donated &quot;) ) Results… Both types of models (discuss here or in forum only…) We present the remaining signed non-zero importance scores from the linear model in the figure below. ( plot_enet_coefs_don_share &lt;- don_share_inc_imp_best_params_recode %&gt;% filter( grepl(&quot;regression&quot;, model, ignore.case = TRUE) &amp; Importance!=0) %&gt;% mutate_labels_sign_snip %&gt;% #mutate(Norm = scale_var(Importance)) %&gt;% mutate(Variable = fct_reorder(Variable, Importance)) %&gt;% ggplot(aes(y = Variable, x = Importance, shape = Sign)) + scale_shape_manual(values = c(95, 43)) + geom_point(size = 2, stroke = 4) + xlab(&quot;Feature importance&quot;) + ggtitle(&quot;Importance scores: predicting log(don.)&quot;) ) Predictive model: Donated 1k USD or more ( iplot_don_1k_best_params &lt;- d_don_1k_best_params_recode %&gt;% filter(!grepl(&quot;tree&quot;, model, ignore.case = TRUE)) %&gt;% mutate_labels_sign_snip %&gt;% norm_vi(slice_top = 10) %&gt;% plot_vi() + ggtitle(&quot;Importance scores: predicting donation &gt; 1k USD &quot;) ) Results… Next, we plot the (nonzero) importance scores for all of the coefficients in the elastic-net logistic model of donating 1k or more. ( plot_enet_coefs_don_1k &lt;- d_don_1k_best_params_recode %&gt;% filter( grepl(&quot;logistic&quot;, model, ignore.case = TRUE) &amp; Importance!=0) %&gt;% mutate_labels_sign_snip %&gt;% #mutate(Norm = scale_var(Importance)) %&gt;% mutate(Variable = fct_reorder(Variable, Importance)) %&gt;% ggplot(aes(y = Variable, x = Importance, shape = Sign)) + scale_shape_manual(values = c(95, 43)) + geom_point(size = 2, stroke = 4) + xlab(&quot;Feature importance&quot;) + ggtitle(&quot;Importance scores: predicting donation &gt; 1000k USD&quot;) ) Model Performance We consider ‘how well these models predict’. Overall, these models offer (…) some predictive power. E.g., note that about 46% of the relevant sample reports over 1k in donations. Our logistic regression model can correctly predict … ] Discussion: measuring prediction success We may want to consider ‘how successful’ our predictive models are at making practically useful predictions. In other words, ‘how far off’ are the predictions and classifications on average, from the actual outcomes. This procedure considers the fit on randomly-drawn set-aside ‘testing data’, data that has not been used in ‘training’ (or ‘fitting’) the model. Below, we consider some commonly-used metrics. Regression Model Performance In order to assess the usefulness of each predictive regression model we consider both root-mean-square-error (RMSE) and mean-absolute-error (MAE). Computing RMSE NOTE: probably move this to the methods book RMSE (aka RMSD) can be interpreted as the average ‘Euclidean distance’ between the actual values and the model’s prediction. For each observation (in the set-aside ‘testing sample’), to construct RMSE we: Measure the differences between the actual and predicted outcome (e.g., donation), 2. Square these differences Take the average of these squared differences across all observations, 4. Take the square root of this Computing MAE NOTE: probably move this to the methods book To construct mean-absolute-error (MAE) we simply 1. Measure the absolute-value differences between the actual and predicted outcome (e.g., donation) for each observation. 2. Take the average of these across all observations MAE has a much more straightforward interpretation: it simply asks ‘how far off are we, on average?’ Use of RMSE vs MAE NOTE: probably move this to the methods book While the RMSE is used in the model fitting for various reasons, it is arguably less-interpretable and less-relevant than MAE in judging the model’s fit in cases like this one. RMSE error negatively assesses the model fit based on squared deviations, and is thus very sensitive to ‘large mistakes’. This may be relevant where ‘large errors are much much worse than small ones’ – here, this is not so clearly the case. In the presence of data with large outlying observations, prediction will tend to be poor by this measure. To address this we: 1. Present both RMSE and MAE 2. Re-run the models of (log average) donations for the subset of individuals with incomes at or below 500,000 USD. For this subset, there are fewer influential donation outliers. The latter also allows us to consider how sensitive the model fit is to outliers. Dealing with transformed outcomes NOTE: probably move this to the methods book Note that when considering models where the outcome is transformed (e.g., log(donations)) we construct the RMSE and MAE by exponentiating to generate predictions for the level outcomes, and then measure the deviations on the level scale. When considering predicted outcomes on the logarithmic scale, both RMSE and MAE indicate roughly ‘how many exponential orders of magnitude our predictions for the non-logged outcomes are off. E.g., a MSE of 1.5 for ’log donation’ suggests an we are off by about \\(exp(1.5) =\\) 4.48 times in terms of donations, getting them off by a factor of about 5. This conversion avoids such complications. l_don_av_2yr_best_params &lt;- l_don_av_2yr_best_params %&gt;% mutate(dv = &quot;Donation amount*&quot;) don_share_inc_imp_best_params &lt;- don_share_inc_imp_best_params %&gt;% mutate(dv = &quot;Donation as a share of income&quot;) ( reg_model_performance &lt;- purrr::map(list(l_don_av_2yr_best_params, don_share_inc_imp_best_params), ~.x %&gt;% dplyr::select(dv, rmse, mae, model, matches(&quot;filter_name&quot;)) ) %&gt;% bind_rows() %&gt;% mutate(filter_name = if_else(is.na(filter_name), &quot;None&quot;, filter_name)) %&gt;% rename(&quot;Dependent variable&quot; = dv, &quot;RMSE&quot; = rmse, &quot;MAE&quot; = mae, &quot;Model&quot; = model, &quot;Filter&quot; = filter_name) %&gt;% kable(caption = &quot;Regression model performance&quot;, digits = 2) %&gt;% kable_styling() %&gt;% add_footnote(&quot;Note: While the model was trained using logs of the dependent variable, RMSE and MAE were calculated in levels&quot;, notation = &quot;symbol&quot;) ) Table 3.11: Regression model performance Dependent variable RMSE MAE Model Filter Donation amount* 66980.31 7259.41 Random Forest None Donation amount* 65011.02 7287.65 Linear Regression (glmnet) None Donation amount* 25944.63 4706.42 Random Forest Income (imp.)_bc5k &lt; 500000 Donation amount* 26010.45 4757.24 Linear Regression (glmnet) Income (imp.)_bc5k &lt; 500000 Donation as a share of income 0.10 0.06 Random Forest None Donation as a share of income 0.11 0.06 Linear Regression (glmnet) None * Note: While the model was trained using logs of the dependent variable, RMSE and MAE were calculated in levels p_load(ie2misc) mad_naive &lt;- ie2misc::madstat(eas_all_s_rl_imp$donation_c, na.rm=TRUE) sd_naive &lt;- round((sd(eas_all_s_rl_imp$donation_c, na.rm=TRUE)), 0) How does this compare to a ‘naive model’ in which we predict the average donation for everyone? Note that for the comparable unfiltered data, the mean absolute deviation is 13,306 and the standard deviation is 106,109. The predictive model reduces this uncertainty substantially (…). Classification Model Performance ROC curve to consider the differences in the predictive power of the various models. Comparison to uninformed classifier, which would simply predict a positive outcome with some random probability \\(p\\) (this maps out the 45 degree line). # Add column for ROC curve roc_curve &lt;- yardstick::roc_curve unnest &lt;- tidyr::unnest pr_curve &lt;- yardstick::pr_curve # Calculate ROC curve d_don_1k_best_params$roc_curve &lt;- d_don_1k_best_params %&gt;% select(true_y, preds, pred_prob, model) %&gt;% unnest(cols = everything()) %&gt;% group_by(model) %&gt;% group_map(~ roc_curve(., true_y, .pred_FALSE)) # Calculate AUC d_don_1k_best_params$auc &lt;- d_don_1k_best_params %&gt;% select(true_y, preds, pred_prob, model) %&gt;% unnest(cols = everything()) %&gt;% group_by(model) %&gt;% group_map(~ yardstick::roc_auc(., truth = true_y, estimate = .pred_FALSE)) # Extract AUC d_don_1k_best_params &lt;- d_don_1k_best_params %&gt;% unnest_wider(., col = auc) %&gt;% select(-c(.metric, .estimator)) %&gt;% rename(auc = .estimate) # Plot ROC curve (roc_curve_d_don_1k &lt;- d_don_1k_best_params %&gt;% select(roc_curve, model) %&gt;% unnest(cols = everything()) %&gt;% rename_with(snakecase::to_title_case) %&gt;% ggplot(aes(x = 1-Specificity, y = Sensitivity, colour = Model)) + geom_line() + geom_abline(slope=1, intercept = 0, linetype = &quot;dotted&quot;) + theme_bw() ) Discussion: ROC curve, AUC NOTE: probably move this to the methods book An ROC curve plots the true positive rate (sensitivity) as a function of the false positive rate (1-specificity). Here the true positive rate gives the rate at which our model correctly predicts a respondent to donate over $1000, with the false positive rate giving the rate at which these predictions are incorrect. Better classifiers will have an ROC curve that is further North-West, with the perfect classifier being an L-shaped curve passing through \\((0,0) \\rightarrow(0,1) \\rightarrow(1,1)\\). Where classifiers ROC curves do not cross, it is clear that one will be performing better than another. That is not the case here. Both models seem to be performing relatively similarly, with the ROC curves overlapping somewhat. It is difficult to discern which model is performing the best, and this will depend on our criterion. However, both models yield curves substantially above the 45 degree line, thus substantially outperforming an uninformed classifier. For example, if we are willing to accept about a 25% rate of false positives (falsely predicting a 1k+ donation), the logistic regression model correctly predicts about 75% of true positives (and the random forest model about 73%). We can use the area under the curve (AUC) measure to compare classifiers for all costs of misclassification. This measure quantifies how close the ROC curve is to the optimal L-shaped curve. calculate_metrics &lt;- function(df, metrics, preds = preds, true_y = true_y){ df %&gt;% mutate(purrr::map2_dfr({{true_y}}, {{preds}}, ~ purrr::map_dfc(metrics, do.call, list(.x, .y)))) } class_metrics &lt;- list(accuracy = yardstick::accuracy_vec, recall = yardstick::recall_vec, precision = yardstick::precision_vec, f1_score = yardstick::f_meas_vec) # Adding a no skill classifier to d_don_1k ## Messy code truth &lt;- d_don_1k_best_params$true_y[[1]] majority &lt;- tail(names(sort(table(truth))), 1) pred_majority &lt;- as.logical(rep(majority, length(truth))) .pred_FALSE &lt;- 1 - pred_majority .pred_TRUE &lt;- 1 - .pred_FALSE pred_prob &lt;- tibble(.pred_FALSE, .pred_TRUE, truth) no_skill &lt;- tibble(model = &quot;No Skill&quot;, true_y = list(truth), pred_prob = list(pred_prob), preds = list(factor(pred_majority, levels = levels(truth)))) %&gt;% calculate_metrics(class_metrics) no_skill$auc &lt;- yardstick::roc_auc_vec(truth, .pred_TRUE) purrr::map_df(list(no_skill, d_don_1k_best_params), ~.x %&gt;% select(model, auc, accuracy)) %&gt;% rename_with(snakecase::to_title_case) %&gt;% rename(AUC = Auc) %&gt;% kable(digits = 3) %&gt;% kable_styling() Model AUC Accuracy No Skill 0.500 0.537 Random Forest 0.817 0.739 Logistic Regression (glmnet) 0.824 0.735 Here we see that the … # Calculate precision recall curve # d_don_1k_best_params$pr_curve &lt;- d_don_1k_best_params %&gt;% select(true_y, preds, pred_prob, model) %&gt;% # unnest(cols = everything()) %&gt;% # group_by(model) %&gt;% # group_map(~ pr_curve(., true_y, .pred_FALSE)) # # d_don_1k_best_params %&gt;% select(pr_curve, model) %&gt;% # unnest(cols = everything()) %&gt;% # ggplot(aes(x = recall, y = precision, colour = model)) + # geom_path() + # coord_equal() + # theme_bw() + # geom_hline(aes(yintercept = 0.46)) best_d_don_1k_preds &lt;- d_don_1k_best_params %&gt;% filter(f1_score == max(f1_score)) %$% preds[[1]] d_don_1k_true &lt;- d_don_1k_best_params %&gt;% filter(f1_score == max(f1_score)) %$% true_y[[1]] #DR: @Oska, I&#39;m just guessing this is what you wanted d_don_1k_preds_df &lt;- tibble(preds = best_d_don_1k_preds, truth = d_don_1k_true) cm &lt;- yardstick::conf_mat(d_don_1k_preds_df, truth, preds) autoplot(cm, type = &quot;heatmap&quot;) + scale_fill_gradient(low=&quot;#D6EAF8&quot;,high = &quot;#2E86C1&quot;) + ggtitle(&quot;Predicting Donations over $1k&quot;) 3.9.3 Summary of modeling results – see introduction 3.10 Appendix: Extra analysis and robustness checks EA Forum Post: “See bookdown appendix for further analysis and robustness checks.” This appendix contains further and analysis and robustness checks, as mentioned or alluded to in the above ‘main text’. This section is part of the ‘Bookdown’ version only and not in the EA forum post. ‘Saved to donate later’ Added 30 Oct 2021 A comment from TylerMaule prompted us to revisit this question. In 2020, after the main donation questions, we also asked Are you presently saving money to donate later, rather than donate now? If so, roughly how much money are you planning to save in 2020 for this purpose? We report on this only briefly here. We did not emphasize it for several reasons. It is unclear how much this responses reflects a strong intention or commitment (e.g., a DAF contribution) that will be manifested in actual later donations. The amount someone reports they are “presently saving … to donate later” could express only a a vague intention to donate late and may be subject to them changing their mind, changes in financial circumstance, or value drift. It is difficult to aggregate this savings with actual donations, as this could lead to double-counting. However, reporting all results for both measures (saving and actual donations) could make this report bulky. However, we hope to look more closely at saving-to-donate behavior in future. This may be particularly relevant for patient longtermists. Key figures eas_new %&lt;&gt;% rowwise() %&gt;% mutate( donate_later_amount_na0 = if_else( is.na(donate_later_amount_c), 0, donate_later_amount_c), donate_later_amount_min50 = max(50, donate_later_amount_c), donate_later_amount_min50_na0 = if_else( is.na(donate_later_amount_min50), 0, donate_later_amount_min50) ) %&gt;% ungroup() #TODO: maybe move above to build side num_savdon &lt;- sum(str_det(eas_new$donate_later, &quot;yes&quot;)==TRUE, na.rm=TRUE) num_na_savdon &lt;- sum(is.na(eas_new$donate_later)==TRUE, na.rm=TRUE) count_eas_new &lt;- NROW(eas_new) med_savdon &lt;- med(eas_new$donate_later_amount_c) mean_savdon &lt;- mean(eas_new$donate_later_amount_c, na.rm=TRUE) mean_savdon0 &lt;- mean(eas_new$donate_later_amount_na0, na.rm=TRUE) med_savdon0 &lt;- med(eas_new$donate_later_amount_na0) mean_savdon0_lt100k &lt;- mean(eas_new$donate_later_amount_na0[eas_new$donate_later_amount_na0&lt;=100000], na.rm=TRUE) 17.9% of responses report saving to donate later. (This represents 24.6% of those who answered this question). Among those who report saving to donate, median donations are 5000 USD and mean donations are 134,267 USD Setting nonresponses to zero the mean ‘saved to donate’ amount (including zeroes) is 17,175 (and the median is 0 of course) We provide a histogram below, with the horizontal axis on a logarithmic scale. #ddate -- adding &#39;saving to donate&#39; amount here #eas_new %&gt;% tabyl(donate_later) %&gt;% .kable() %&gt;% .kable_styling() ( sav_don_ty &lt;- eas_new %&gt;% hist_plot_lscale(eas_new$donate_later_amount_min50, breaks = breaks) + labs(title=&quot;Histogram of &#39;saved to to donate later, 2020, bottom-coded at 50&#39;&quot;, x = &quot;Amount in USD&quot;, y = &quot;Number of respondents&quot;)) Donations by engagement level don_share_income_by_engage_sp Donations and income by student/employment status Below, we tabulate income and donations, split by student and employment status. don_inc_by_student html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #swirghxfwt .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #swirghxfwt .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #swirghxfwt .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #swirghxfwt .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; border-top-color: #FFFFFF; border-top-width: 0; } #swirghxfwt .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #swirghxfwt .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #swirghxfwt .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #swirghxfwt .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #swirghxfwt .gt_column_spanner_outer:first-child { padding-left: 0; } #swirghxfwt .gt_column_spanner_outer:last-child { padding-right: 0; } #swirghxfwt .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #swirghxfwt .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #swirghxfwt .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #swirghxfwt .gt_from_md > :first-child { margin-top: 0; } #swirghxfwt .gt_from_md > :last-child { margin-bottom: 0; } #swirghxfwt .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #swirghxfwt .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #swirghxfwt .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #swirghxfwt .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #swirghxfwt .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #swirghxfwt .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #swirghxfwt .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #swirghxfwt .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #swirghxfwt .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #swirghxfwt .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #swirghxfwt .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #swirghxfwt .gt_sourcenote { font-size: 90%; padding: 4px; } #swirghxfwt .gt_left { text-align: left; } #swirghxfwt .gt_center { text-align: center; } #swirghxfwt .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #swirghxfwt .gt_font_normal { font-weight: normal; } #swirghxfwt .gt_font_bold { font-weight: bold; } #swirghxfwt .gt_font_italic { font-style: italic; } #swirghxfwt .gt_super { font-size: 65%; } #swirghxfwt .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 65%; } Characteristic N Overall, N = 1,6361 employed_ft, N = 707 employed_pt, N = 70 not_employed_looking, N = 57 Other, N = 120 self_employed, N = 127 student_doctoral, N = 119 student_masters, N = 155 student_undergrad, N = 281 Income in $1000 USD 1,400 Median 33 60 33 15 24 42 26 12 4 10%-90% 1-130 14-182 6-70 1-92 0-107 10-126 6-55 0-56 0-17 Mean [se] (SD) 61 [4] (138) 93 [6] (144) 43 [6] (48) 37 [8] (54) 52 [17] (160) 87 [28] (290) 28 [2] (18) 22 [3] (34) 8 [1] (9) Last year's donation (in USD) 1,413 Median 528 1,420 679 284 500 1,247 548 178 110 10%-90% 0-9,822 0-17,048 0-5,975 0-4,200 0-9,000 0-15,249 0-3,793 0-2,367 0-1,000 Mean [se] (SD) 7,516 [1,944] (73,062) 10,048 [1,769] (44,507) 2,779 [743] (5,944) 3,541 [2,072] (14,503) 4,085 [1,158] (11,048) 28,589 [22,709] (238,179) 1,505 [209] (2,165) 953 [199] (2,329) 450 [77] (1,141) Latest planned donation 1,393 Median 1,000 3,052 1,319 349 578 1,751 1,500 327 200 10%-90% 0-12,039 0-21,500 0-7,167 0-5,114 0-8,085 0-12,534 0-5,678 0-2,869 0-2,000 Mean [se] (SD) 10,002 [1,993] (74,389) 14,473 [2,895] (72,421) 3,352 [789] (6,314) 5,990 [4,166] (28,864) 10,903 [5,619] (52,712) 26,841 [19,090] (196,545) 2,655 [480] (4,919) 990 [163] (1,906) 691 [93] (1,372) 1 c(&quot;Median&quot;, &quot;10%-90%&quot;, &quot;Mean [se] (SD)&quot;) # don_inc_by_student %&gt;% as_image(width = 8) Donation as share of income by tenure, ‘faceted’ by referrer to survey* ( don_share_by_tenure_facet_referrer &lt;- eas_new %&gt;% filter(!is.na(age_approx_ranges)) %&gt;% ggplot() + aes(x = tenure, y = don_share_inc_imp) + geom_point(size = 0.15, colour = &quot;#0c4c8a&quot;, position = position_jitter(seed = 42, width = 0.1, height = 0.001)) + geom_smooth(span = 0.75) + scatter_theme + facet_grid(vars(), vars(referrer_cat2), scales = &quot;free&quot;) + labs(title = &quot;2019 donation as share of (imputed) income by time in EA faceted by referrer category&quot;) + labs(x = get_label(eas_new$referrer_cat2)) + ylim(0, 0.3) ) %&gt;% ggplotly For several major groups of referrers, we (again) see a strong positive association between time-in-EA and donations as a share of income. Additional: Donations and income by whether a Longtermist cause is top priority We were asked to compare the donations of those who prioritize longtermist causes to the remainder of EAs. Below, we tabulate this by whether a respondent gives some longtermist cause as high a priority rating as any other cause. #eas_new %&gt;% tabyl(lt_above_mn_priority) #lt_4plus_priority = case_when( don_income_by_priority &lt;- eas_new %&gt;% dplyr::select(income_k_c, donation_c, donation_plan_c, don_share_inc_imp, lt_top_priority) %&gt;% tbl_summary( by = lt_top_priority, type = c(all_continuous()) ~ &quot;continuous2&quot;, statistic = list(all_continuous() ~ sumstatvec), label = list(income_k_c ~ &quot;Income in $1000 USD&quot;, donation_c ~ &quot;Last year&#39;s donation (in USD)&quot;, donation_plan_c ~ &quot;Latest planned donation&quot;, don_share_inc_imp ~ &quot;Last year&#39;s donation as share of (imputed) income&quot; ), missing = c(&quot;no&quot;) ) %&gt;% bold_labels() %&gt;% add_n() %&gt;% add_overall() don_income_by_priority html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #agrbfqbnoo .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #agrbfqbnoo .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #agrbfqbnoo .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #agrbfqbnoo .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; border-top-color: #FFFFFF; border-top-width: 0; } #agrbfqbnoo .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #agrbfqbnoo .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #agrbfqbnoo .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #agrbfqbnoo .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #agrbfqbnoo .gt_column_spanner_outer:first-child { padding-left: 0; } #agrbfqbnoo .gt_column_spanner_outer:last-child { padding-right: 0; } #agrbfqbnoo .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #agrbfqbnoo .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #agrbfqbnoo .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #agrbfqbnoo .gt_from_md > :first-child { margin-top: 0; } #agrbfqbnoo .gt_from_md > :last-child { margin-bottom: 0; } #agrbfqbnoo .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #agrbfqbnoo .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #agrbfqbnoo .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #agrbfqbnoo .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #agrbfqbnoo .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #agrbfqbnoo .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #agrbfqbnoo .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #agrbfqbnoo .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #agrbfqbnoo .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #agrbfqbnoo .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #agrbfqbnoo .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #agrbfqbnoo .gt_sourcenote { font-size: 90%; padding: 4px; } #agrbfqbnoo .gt_left { text-align: left; } #agrbfqbnoo .gt_center { text-align: center; } #agrbfqbnoo .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #agrbfqbnoo .gt_font_normal { font-weight: normal; } #agrbfqbnoo .gt_font_bold { font-weight: bold; } #agrbfqbnoo .gt_font_italic { font-style: italic; } #agrbfqbnoo .gt_super { font-size: 65%; } #agrbfqbnoo .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 65%; } Characteristic N Overall, N = 1,6161 FALSE, N = 796 TRUE, N = 820 Income in $1000 USD 1,361 Median 34 38 29 10%-90% 1-130 2-129 0-130 Mean [se] (SD) 61 [4] (138) 59 [4] (111) 62 [6] (161) Last year's donation (in USD) 1,374 Median 559 700 500 10%-90% 0-10,000 0-10,000 0-10,000 Mean [se] (SD) 7,688 [1,998] (74,077) 6,212 [1,289] (33,823) 9,172 [3,794] (99,297) Latest planned donation 1,354 Median 1,053 1,320 939 10%-90% 0-12,759 0-12,049 0-12,796 Mean [se] (SD) 10,186 [2,050] (75,431) 9,446 [2,188] (57,095) 10,935 [3,481] (90,311) Last year's donation as share of (imputed) income 1,373 Median 0.02 0.03 0.02 10%-90% 0.00-0.14 0.00-0.15 0.00-0.14 Mean [se] (SD) 0.13 [0.05] (1.82) 0.09 [0.01] (0.33) 0.16 [0.10] (2.55) 1 c(&quot;Median&quot;, &quot;10%-90%&quot;, &quot;Mean [se] (SD)&quot;) # don_income_by_priority %&gt;% as_image(width = 8) # Todo (medium): fix labeling and titles above # Todo (High): add standard error and perhaps tests, perhaps Bayes factors # Todo (Medium-high): visualization of donations by top-priority category (poverty, animals, LT, meta/other) don_inc_priority_plot &lt;- eas_new %&gt;% grp_sum(income_c_imp_bc5k, donation_c, XXX) %&gt;% plot_grp(country_big) + xlab(&quot;Mean income in USD (imputed if &lt;5k/missing&quot;) + ylab(&quot;Mean donations, CIs&quot;) The above presentation is meant to be broadly descriptive. Overall, it appears that those who prioritize long-term causes tend to donate a bit less at median but more at mean, and there is a lot of overlap between the two groups. 3.10.1 Planned donation by year, comparison tests We report the amount people say they expect to donate in the current (survey) year below. Note the high share of missing values for 2017, and the substantially-larger sample in 2018. As noted above, this suggests that comparing the planned donations for 2018 (in EA survey 2018) to actual donations for 2018 (reported in 2019 EA survey) may not be informative. ( plan_don_per_year &lt;- eas_all %&gt;% filter(year&gt;=2017) %&gt;% group_by(year) %&gt;% summarise(&quot;Number reported&quot; = sum(!is.na(donation_plan_c)), N = n(), &quot;Number missing&quot; = sum(is.na(donation_plan_c)), &quot;Proportion missing (%)&quot; = sum(is.na(donation_plan_c))/n()*100, &quot;Mean planned donation&quot; = mean(donation_plan_c, na.rm=TRUE), &quot;Median planned donation&quot; = median(donation_plan_c, na.rm=TRUE)) %&gt;% kable(caption = &quot;Planned donation per year&quot;) %&gt;% kable_styling() ) (#tab:plan_don_per_year)Planned donation per year year Number reported N Number missing Proportion missing (%) Mean planned donation Median planned donation 2017 831 1845 1014 54.95935 1806.744 10.00 2018 1790 2599 809 31.12736 11045.812 1156.22 2019 1678 2509 831 33.12077 10704.457 1088.80 2020 1402 2056 654 31.80934 9992.946 1000.00 Below, the results of the signed rank test for plan versus actual donation, excluding 0 donations. w_signed_test_planned_actual_no0s ## ## Wilcoxon signed rank test with continuity correction ## ## data: planned_actual_ly_ty_no_0$donation_last_survey_year and planned_actual_ly_ty_no_0$planned_donation_prior_survey_year ## V = 23282, p-value = 0.00000009715 ## alternative hypothesis: true location shift is greater than 0 ## 95 percent confidence interval: ## 271.685 Inf ## sample estimates: ## (pseudo)median ## 425.7262 3.10.2 Donations by income ‘earning-to-give’ career status ( don_income_etg &lt;- eas_all %&gt;% filter(year_n&gt;=2018) %&gt;% ggplot(aes(x = income_c_imp_bc_k, y = donation_2019, color = d_career_etg)) + geom_point(size = 1, alpha = 0.7) + # draw the points geom_smooth(aes(method = &#39;loess&#39;, fill = d_career_etg)) + # @Oska -- note I am using local smoothing here. scale_x_log10(name = &quot;Income in $1K USD (imputed if &lt;5k/missing)&quot;, n.breaks = 5, limits = c(5, 5000)) + scale_y_log10( name = &quot;Donation amount&quot;, # labels = scales::dollar, labels = scales::label_number_si(prefix = &quot;$&quot;), n.breaks = 10 ) + scale_color_discrete(name = &quot;Earning to give&quot;) + scale_fill_discrete(guide = &quot;none&quot;) + theme(axis.text.x = element_text( angle = 90, vjust = 0.5, hjust = 1 ), legend.position = c(.87,.15), legend.background = element_rect(fill=alpha(&#39;blue&#39;, 0.01))) ) #don_income_etg + facet_wrap(~d_gwwc_ever) 3.10.3 Donation shares by income (by US-residence, 2019) By request, we consider median and mean shares of income donated, separately tallying for US residents and others. This allows us to compare donation rates to those reported for the US overall. In particular, Meer and Priday (2020) reports a roughly 1.6-2.1% percent share of income donated throughout all of the US income quantiles (except or the lowest 5%). The statistics below suggest that EAs who fill out the donation questions in the survey to donate a greater share of their income, perhaps at least twice as much. Recall that the mean share of total (imputed) income donated (for 2019) was 9.4% (imputing income where below 5k or missing). If we focus on US-resident nonstudents across all years, the mean share is 7.94%. mean_don_sharenonstud_usa_w20 &lt;- eas_all %&gt;% filter(d_live_usa==1 &amp; d_student==0) %&gt;% rowwise() %&gt;% mutate(don_share_inc_imp_bc5k = min(don_share_inc_imp_bc5k, 0.2)) %&gt;% ungroup() %&gt;% summarize(mean_share_nonstud_usa_w20 = mean(don_share_inc_imp_bc5k, na.rm=TRUE) ) We could also ‘average the individual shares donated’; as this is highly sensitive to outliers (e.g., people with the lowest incomes reporting donating many times this share), we ‘Winsorise’, capping each individual’s ‘share of income donate’ at some percent. Following this procedure and capping at 20%, the mean share of income donated, across all years, for US-resident non-students in the EA survey, is 5.67%. We plot donation shares on a log scale, for US and non-US residents separately (across all years). Below, the blue dash gives us the median for USA (vs non-USA), and the smoothed curve tries to best fit the mean. The left graph is ‘Winsorised’ at 20%, and right one at 40% don_share_income_by_usa &lt;- eas_all %&gt;% filter(!is.na(d_live_usa)) %&gt;% rowwise() %&gt;% mutate(don_share_inc_imp_bc5k = min(don_share_inc_imp_bc5k, 0.4)) %&gt;% ungroup() %&gt;% group_by(d_live_usa) %&gt;% mutate(med_usa = median(don_share_inc_imp_bc5k, na.rm=TRUE)) %&gt;% ungroup() %&gt;% ggplot(aes(x = income_c_imp_bc_k, y = don_share_inc_imp_bc5k)) + ggpointdensity::geom_pointdensity(adjust = 0.25) + geom_smooth(method = &quot;loess&quot;) + geom_hline(aes(yintercept=med_usa), linetype=&quot;dashed&quot;, size=0.5, color = &quot;blue&quot;) + geom_hline(yintercept=0.1, linetype=&quot;dashed&quot;, size=0.5, color = &quot;red&quot;) + scale_y_continuous(labels = scales::label_percent(accuracy = 1L)) + scale_x_log10(breaks = scales::log_breaks(n=7)) + scale_color_viridis_c(&quot;Neighbours&quot;) + xlab(&quot;Income in $1K USD (imputed if missing, bottom-code at 5k)&quot;) + theme(axis.title.x = element_text(size = 10)) + ylim(NA,.21) + ylab(&quot;Donations/Income (top-code at 40%)&quot;) + facet_wrap(~d_live_usa) + labs(title=&quot;By US residence: 2019 &#39;Don. share of income&#39; by income (w/ imputing)&quot;) don_share_income_by_usa_w20 &lt;- eas_all %&gt;% filter(!is.na(d_live_usa)) %&gt;% rowwise() %&gt;% mutate(don_share_inc_imp_bc5k = min(don_share_inc_imp_bc5k, 0.2)) %&gt;% ungroup() %&gt;% group_by(d_live_usa) %&gt;% mutate(med_usa = median(don_share_inc_imp_bc5k, na.rm=TRUE)) %&gt;% ungroup() %&gt;% ggplot(aes(x = income_c_imp_bc_k, y = don_share_inc_imp_bc5k)) + ggpointdensity::geom_pointdensity(adjust = 0.25) + geom_smooth(method = &quot;loess&quot;) + geom_hline(aes(yintercept=med_usa), linetype=&quot;dashed&quot;, size=0.5, color = &quot;blue&quot;) + geom_hline(yintercept=0.1, linetype=&quot;dashed&quot;, size=0.5, color = &quot;red&quot;) + scale_y_continuous(labels = scales::label_percent(accuracy = 1L)) + scale_x_log10(breaks = scales::log_breaks(n=7)) + scale_color_viridis_c(&quot;Neighbours&quot;) + xlab(&quot;Income in $1K USD (imputed if missing, bottom-code at 5k)&quot;) + theme(axis.title.x = element_text(size = 10)) + ylab(&quot;Donations/Income (&#39;Windsorised&#39; (top-coded) at 40%)&quot;) + ylim(NA,.21) + facet_wrap(~d_live_usa) + labs(title=&quot;By US residence: 2019 &#39;Don. share of income&#39; by income (w/ imputing)&quot;) mean_don_share_income_usa_nonstudent_w20 &lt;- eas_all %&gt;% filter(d_live_usa==1 &amp; d_student==0) %&gt;% rowwise() %&gt;% mutate(don_share_inc_imp_bc5k = min(don_share_inc_imp_bc5k, 0.2)) %&gt;% ungroup() %&gt;% dplyr::summarise(mean_don_share= mean(don_share_inc_imp_bc5k, na.rm=TRUE) ) #don_income_etg + facet_wrap(~d_gwwc_ever) 3.10.4 Models: nonresponse coefficients Below we report the ‘no response’ coefficients for all of the models presented above (recall that these are ‘controls’ included in each specification). These are exponentiated, as in the presentations above. We report these only here because we suspect they are of less direct interest. # make nice table (and/or forest plot?) of nonreseponses ( nr_coefs_table &lt;- bind_rows(nr_coefs_qp, nr_coefs_fl,nr_coefs_logit) %&gt;% select(outcome, term, model_name, estimate, std.error, p.value, conf.low, conf.high) %&gt;% filter(!str_detect(model_name, &quot;GWWC&quot;)) %&gt;% filter(str_detect(term, &quot;response|na|NA&quot;)) %&gt;% mutate( outcome = str_replace_all(outcome, key_eas_all_labels) ) %&gt;% arrange(term) %&gt;% .kable(digits=3) %&gt;% .kable_styling() ) outcome term model_name estimate std.error p.value conf.low conf.high Don. ‘avg’ City: No response Baseline 0.639 0.176 0.011 0.452 0.902 Don. ‘avg’ City: No response Robust controls 0.627 0.175 0.008 0.444 0.884 Don./Income (imp, bc) City: No response Baseline 0.827 0.195 0.331 0.564 1.212 Don./Income (imp, bc) City: No response Robust controls 0.825 0.196 0.326 0.562 1.211 Don. &gt; 1k USD City: No response Baseline 0.979 0.180 0.905 0.687 1.394 Don. &gt; 1k USD City: No response Robust controls 0.933 0.187 0.713 0.647 1.347 Don. ‘avg’ Gender: No response Baseline 0.833 0.262 0.486 0.499 1.392 Don. ‘avg’ Gender: No response Robust controls 0.847 0.265 0.532 0.504 1.425 Don./Income (imp, bc) Gender: No response Baseline 0.996 0.211 0.986 0.658 1.508 Don./Income (imp, bc) Gender: No response Robust controls 0.998 0.212 0.992 0.659 1.511 Don. &gt; 1k USD Gender: No response Baseline 0.860 0.226 0.505 0.553 1.339 Don. &gt; 1k USD Gender: No response Robust controls 0.956 0.234 0.848 0.604 1.513 Don. ‘avg’ Live: Country: No response Baseline 1.309 0.274 0.326 0.765 2.242 Don. ‘avg’ Live: Country: No response Robust controls 1.308 0.279 0.336 0.757 2.258 Don./Income (imp, bc) Live: Country: No response Baseline 1.332 0.284 0.313 0.763 2.323 Don./Income (imp, bc) Live: Country: No response Robust controls 1.335 0.285 0.311 0.764 2.332 Don. &gt; 1k USD Live: Country: No response Baseline 1.094 0.200 0.655 0.738 1.620 Don. &gt; 1k USD Live: Country: No response Robust controls 1.144 0.208 0.517 0.761 1.721 Don. ‘avg’ NA Baseline 0.817 0.378 0.592 0.389 1.713 Don. ‘avg’ NA Robust controls 0.817 0.385 0.599 0.384 1.738 Don./Income (imp, bc) NA Baseline 0.844 0.169 0.317 0.606 1.176 Don./Income (imp, bc) NA Robust controls 0.842 0.169 0.310 0.604 1.174 Don. &gt; 1k USD NA Baseline 0.854 0.190 0.407 0.588 1.240 Don. &gt; 1k USD NA Robust controls 0.781 0.201 0.219 0.526 1.158 Don. ‘avg’ Student: No response Baseline 0.236 0.862 0.094 0.044 1.277 Don. ‘avg’ Student: No response Robust controls 0.245 0.753 0.062 0.056 1.072 Don./Income (imp, bc) Student: No response Baseline 0.350 0.381 0.006 0.166 0.737 Don./Income (imp, bc) Student: No response Robust controls 0.351 0.379 0.006 0.167 0.738 Don. &gt; 1k USD Student: No response Baseline 1.090 0.761 0.910 0.245 4.845 Don. &gt; 1k USD Student: No response Robust controls 1.164 0.740 0.838 0.273 4.958 Don. ‘avg’ Top-6 Uni: NA Baseline 1.083 0.338 0.814 0.558 2.100 Don. ‘avg’ Top-6 Uni: NA Robust controls 1.078 0.337 0.823 0.557 2.086 Don./Income (imp, bc) Top-6 Uni: NA Baseline 0.775 0.120 0.034 0.612 0.980 Don./Income (imp, bc) Top-6 Uni: NA Robust controls 0.778 0.120 0.037 0.615 0.985 Don. &gt; 1k USD Top-6 Uni: NA Baseline 0.777 0.127 0.046 0.606 0.995 Don. &gt; 1k USD Top-6 Uni: NA Robust controls 0.819 0.130 0.124 0.634 1.057 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
